{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "#v2\n",
    "#28/11/2018\n",
    "\n",
    "dataname=\"epistroma\"\n",
    "ignore_index = -100 #Unet has the possibility of masking out pixels in the output image, we can specify the index value here (though not used)\n",
    "gpuid=0\n",
    "\n",
    "# --- unet params\n",
    "#these parameters get fed directly into the UNET class, and more description of them can be discovered there\n",
    "n_classes= 2    #number of classes in the data mask that we'll aim to predict\n",
    "in_channels= 3  #input channel of the data, RGB = 3\n",
    "padding= True   #should levels be padded\n",
    "depth= 5       #depth of the network \n",
    "wf= 2           #wf (int): number of filters in the first layer is 2**wf, was 6\n",
    "up_mode= 'upconv' #should we simply upsample the mask, or should we try and learn an interpolation \n",
    "batch_norm = True #should we use batch normalization between the layers\n",
    "\n",
    "# --- training params\n",
    "batch_size=3\n",
    "patch_size=256\n",
    "num_epochs = 100\n",
    "edge_weight = 1.1 #edges tend to be the most poorly segmented given how little area they occupy in the training set, this paramter boosts their values along the lines of the original UNET paper\n",
    "phases = [\"train\",\"val\"] #how many phases did we create databases for?\n",
    "validation_phases= [\"val\"] #when should we do valiation? note that validation is time consuming, so as opposed to doing for both training and validation, we do it only for vlaidation at the end of the epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "\n",
    "import PIL\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "import numpy as np\n",
    "import sys, glob\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "import scipy.ndimage \n",
    "\n",
    "import time\n",
    "import math\n",
    "import tables\n",
    "\n",
    "import random\n",
    "\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels=1,\n",
    "        n_classes=2,\n",
    "        depth=5,\n",
    "        wf=6,\n",
    "        padding=False,\n",
    "        batch_norm=False,\n",
    "        up_mode='upconv',\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Implementation of\n",
    "        U-Net: Convolutional Networks for Biomedical Image Segmentation\n",
    "        (Ronneberger et al., 2015)\n",
    "        https://arxiv.org/abs/1505.04597\n",
    "        Using the default arguments will yield the exact version used\n",
    "        in the original paper\n",
    "        Args:\n",
    "            in_channels (int): number of input channels\n",
    "            n_classes (int): number of output channels\n",
    "            depth (int): depth of the network\n",
    "            wf (int): number of filters in the first layer is 2**wf\n",
    "            padding (bool): if True, apply padding such that the input shape\n",
    "                            is the same as the output.\n",
    "                            This may introduce artifacts\n",
    "            batch_norm (bool): Use BatchNorm after layers with an\n",
    "                               activation function\n",
    "            up_mode (str): one of 'upconv' or 'upsample'.\n",
    "                           'upconv' will use transposed convolutions for\n",
    "                           learned upsampling.\n",
    "                           'upsample' will use bilinear upsampling.\n",
    "        \"\"\"\n",
    "        super(UNet, self).__init__()\n",
    "        assert up_mode in ('upconv', 'upsample')\n",
    "        self.padding = padding\n",
    "        self.depth = depth\n",
    "        prev_channels = in_channels\n",
    "        self.down_path = nn.ModuleList()\n",
    "        for i in range(depth):\n",
    "            self.down_path.append(\n",
    "                UNetConvBlock(prev_channels, 2 ** (wf + i), padding, batch_norm)\n",
    "            )\n",
    "            prev_channels = 2 ** (wf + i)\n",
    "\n",
    "        self.up_path = nn.ModuleList()\n",
    "        for i in reversed(range(depth - 1)):\n",
    "            self.up_path.append(\n",
    "                UNetUpBlock(prev_channels, 2 ** (wf + i), up_mode, padding, batch_norm)\n",
    "            )\n",
    "            prev_channels = 2 ** (wf + i)\n",
    "\n",
    "        self.last = nn.Conv2d(prev_channels, n_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        blocks = []\n",
    "        for i, down in enumerate(self.down_path):\n",
    "            x = down(x)\n",
    "            if i != len(self.down_path) - 1:\n",
    "                blocks.append(x)\n",
    "                x = F.max_pool2d(x, 2)\n",
    "\n",
    "        for i, up in enumerate(self.up_path):\n",
    "            x = up(x, blocks[-i - 1])\n",
    "\n",
    "        return self.last(x)\n",
    "\n",
    "\n",
    "class UNetConvBlock(nn.Module):\n",
    "    def __init__(self, in_size, out_size, padding, batch_norm):\n",
    "        super(UNetConvBlock, self).__init__()\n",
    "        block = []\n",
    "\n",
    "        block.append(nn.Conv2d(in_size, out_size, kernel_size=3, padding=int(padding)))\n",
    "        block.append(nn.ReLU())\n",
    "        if batch_norm:\n",
    "            block.append(nn.BatchNorm2d(out_size))\n",
    "\n",
    "        block.append(nn.Conv2d(out_size, out_size, kernel_size=3, padding=int(padding)))\n",
    "        block.append(nn.ReLU())\n",
    "        if batch_norm:\n",
    "            block.append(nn.BatchNorm2d(out_size))\n",
    "\n",
    "        self.block = nn.Sequential(*block)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.block(x)\n",
    "        return out\n",
    "\n",
    "\n",
    "class UNetUpBlock(nn.Module):\n",
    "    def __init__(self, in_size, out_size, up_mode, padding, batch_norm):\n",
    "        super(UNetUpBlock, self).__init__()\n",
    "        if up_mode == 'upconv':\n",
    "            self.up = nn.ConvTranspose2d(in_size, out_size, kernel_size=2, stride=2)\n",
    "        elif up_mode == 'upsample':\n",
    "            self.up = nn.Sequential(\n",
    "                nn.Upsample(mode='bilinear', scale_factor=2),\n",
    "                nn.Conv2d(in_size, out_size, kernel_size=1),\n",
    "            )\n",
    "\n",
    "        self.conv_block = UNetConvBlock(in_size, out_size, padding, batch_norm)\n",
    "\n",
    "    def center_crop(self, layer, target_size):\n",
    "        _, _, layer_height, layer_width = layer.size()\n",
    "        diff_y = (layer_height - target_size[0]) // 2\n",
    "        diff_x = (layer_width - target_size[1]) // 2\n",
    "        return layer[\n",
    "            :, :, diff_y : (diff_y + target_size[0]), diff_x : (diff_x + target_size[1])\n",
    "        ]\n",
    "\n",
    "    def forward(self, x, bridge):\n",
    "        up = self.up(x)\n",
    "        crop1 = self.center_crop(bridge, up.shape[2:])\n",
    "        out = torch.cat([up, crop1], 1)\n",
    "        out = self.conv_block(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper function for pretty printing of current time and remaining time\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent+.00001)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_CudaDeviceProperties(name='GeForce GTX 1050 Ti', major=6, minor=1, total_memory=4096MB, multi_processor_count=6)\n"
     ]
    }
   ],
   "source": [
    "#specify if we should use a GPU (cuda) or only the CPU\n",
    "if(torch.cuda.is_available()):\n",
    "    print(torch.cuda.get_device_properties(gpuid))\n",
    "    torch.cuda.set_device(gpuid)\n",
    "    device = torch.device(f'cuda:{gpuid}')\n",
    "else:\n",
    "    device = torch.device(f'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total params: \t122466\n"
     ]
    }
   ],
   "source": [
    "#build the model according to the paramters specified above and copy it to the GPU. finally print out the number of trainable parameters\n",
    "model = UNet(n_classes=n_classes, in_channels=in_channels, padding=padding,depth=depth,wf=wf, up_mode=up_mode, batch_norm=batch_norm).to(device)\n",
    "print(f\"total params: \\t{sum([np.prod(p.size()) for p in model.parameters()])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this defines our dataset class which will be used by the dataloader\n",
    "class Dataset(object):\n",
    "    def __init__(self, fname ,img_transform=None, mask_transform = None, edge_weight= False):\n",
    "        #nothing special here, just internalizing the constructor parameters\n",
    "        self.fname=fname\n",
    "        self.edge_weight = edge_weight\n",
    "        \n",
    "        self.img_transform=img_transform\n",
    "        self.mask_transform = mask_transform\n",
    "        \n",
    "        self.tables=tables.open_file(self.fname)\n",
    "        self.numpixels=self.tables.root.numpixels[:]\n",
    "        self.nitems=self.tables.root.img.shape[0]\n",
    "        self.tables.close()\n",
    "        \n",
    "        self.img = None\n",
    "        self.mask = None\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        #opening should be done in __init__ but seems to be\n",
    "        #an issue with multithreading so doing here\n",
    "        with tables.open_file(self.fname,'r') as db:\n",
    "            self.img=db.root.img\n",
    "            self.mask=db.root.mask\n",
    "       \n",
    "            #get the requested image and mask from the pytable\n",
    "            img = self.img[index,:,:,:]\n",
    "            mask = self.mask[index,:,:]\n",
    "        \n",
    "        #the original Unet paper assignes increased weights to the edges of the annotated objects\n",
    "        #their method is more sophistocated, but this one is faster, we simply dilate the mask and \n",
    "        #highlight all the pixels which were \"added\"\n",
    "        if(self.edge_weight):\n",
    "            weight = scipy.ndimage.morphology.binary_dilation(mask==1, iterations =2) & ~mask\n",
    "        else: #otherwise the edge weight is all ones and thus has no affect\n",
    "            weight = np.ones(mask.shape,dtype=mask.dtype)\n",
    "        \n",
    "        mask = mask[:,:,None].repeat(3,axis=2) #in order to use the transformations given by torchvision\n",
    "        weight = weight[:,:,None].repeat(3,axis=2) #inputs need to be 3D, so here we convert from 1d to 3d by repetition\n",
    "        \n",
    "        img_new = img\n",
    "        mask_new = mask\n",
    "        weight_new = weight\n",
    "        \n",
    "        seed = random.randrange(sys.maxsize) #get a random seed so that we can reproducibly do the transofrmations\n",
    "        if self.img_transform is not None:\n",
    "            random.seed(seed) # apply this seed to img transforms\n",
    "            img_new = self.img_transform(img)\n",
    "\n",
    "        if self.mask_transform is not None:\n",
    "            random.seed(seed)\n",
    "            mask_new = self.mask_transform(mask)\n",
    "            mask_new = np.asarray(mask_new)[:,:,0].squeeze()\n",
    "            \n",
    "            random.seed(seed)\n",
    "            weight_new = self.mask_transform(weight)\n",
    "            weight_new = np.asarray(weight_new)[:,:,0].squeeze()\n",
    "\n",
    "        return img_new, mask_new, weight_new\n",
    "    def __len__(self):\n",
    "        return self.nitems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "``./epistroma_train.pytable`` does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-b4bf20fe7de9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     31\u001b[0m                      \u001b[1;31m#interestingly, given the batch size, i've not seen any improvements from using a num_workers>0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m     \u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mphase\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"./{dataname}_{phase}.pytable\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg_transform\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mimg_transform\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mmask_transform\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmask_transform\u001b[0m \u001b[1;33m,\u001b[0m\u001b[0medge_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0medge_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m     dataLoader[phase]=DataLoader(dataset[phase], batch_size=batch_size, \n\u001b[0;32m     35\u001b[0m                                 shuffle=True, num_workers=6, pin_memory=True) \n",
      "\u001b[1;32m<ipython-input-12-43983316c6c9>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, fname, img_transform, mask_transform, edge_weight)\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmask_transform\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmask_transform\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtables\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtables\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpixels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtables\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpixels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnitems\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtables\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tables\\file.py\u001b[0m in \u001b[0;36mopen_file\u001b[1;34m(filename, mode, title, root_uep, filters, **kwargs)\u001b[0m\n\u001b[0;32m    318\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    319\u001b[0m     \u001b[1;31m# Finally, create the File instance, and return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 320\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mroot_uep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    321\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    322\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tables\\file.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, filename, mode, title, root_uep, filters, **kwargs)\u001b[0m\n\u001b[0;32m    782\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    783\u001b[0m         \u001b[1;31m# Now, it is time to initialize the File extension\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 784\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_g_new\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    785\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    786\u001b[0m         \u001b[1;31m# Check filters and set PyTables format version for new files.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mtables/hdf5extension.pyx\u001b[0m in \u001b[0;36mtables.hdf5extension.File._g_new\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tables\\utils.py\u001b[0m in \u001b[0;36mcheck_file_access\u001b[1;34m(filename, mode)\u001b[0m\n\u001b[0;32m    155\u001b[0m         \u001b[1;31m# The file should be readable.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maccess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mF_OK\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 157\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"``%s`` does not exist\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    158\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    159\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"``%s`` is not a regular file\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: ``./epistroma_train.pytable`` does not exist"
     ]
    }
   ],
   "source": [
    "#note that since we need the transofrmations to be reproducible for both masks and images\n",
    "#we do the spatial transformations first, and afterwards do any color augmentations\n",
    "img_transform = transforms.Compose([\n",
    "     transforms.ToPILImage(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(size=(patch_size,patch_size),pad_if_needed=True), #these need to be in a reproducible order, first affine transforms and then color\n",
    "    transforms.RandomResizedCrop(size=patch_size),\n",
    "    transforms.RandomRotation(180),\n",
    "    transforms.ColorJitter(brightness=0, contrast=0, saturation=0, hue=.5),\n",
    "    transforms.RandomGrayscale(),\n",
    "    transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "\n",
    "mask_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(size=(patch_size,patch_size),pad_if_needed=True), #these need to be in a reproducible order, first affine transforms and then color\n",
    "    transforms.RandomResizedCrop(size=patch_size,interpolation=PIL.Image.NEAREST),\n",
    "    transforms.RandomRotation(180),\n",
    "    ])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dataset={}\n",
    "dataLoader={}\n",
    "for phase in phases: #now for each of the phases, we're creating the dataloader\n",
    "                     #interestingly, given the batch size, i've not seen any improvements from using a num_workers>0\n",
    "    \n",
    "    dataset[phase]=Dataset(f\"./{dataname}_{phase}.pytable\", img_transform=img_transform , mask_transform = mask_transform ,edge_weight=edge_weight)\n",
    "    dataLoader[phase]=DataLoader(dataset[phase], batch_size=batch_size, \n",
    "                                shuffle=True, num_workers=6, pin_memory=True) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize a single example to verify that it is correct\n",
    "(img,patch_mask,patch_mask_weight)=dataset[\"train\"][7]\n",
    "fig, ax = plt.subplots(1,4, figsize=(10,4))  # 1 row, 2 columns\n",
    "\n",
    "#build output showing original patch  (after augmentation), class = 1 mask, weighting mask, overall mask (to see any ignored classes)\n",
    "ax[0].imshow(np.moveaxis(img.numpy(),0,-1))\n",
    "ax[1].imshow(patch_mask==1)\n",
    "ax[2].imshow(patch_mask_weight)\n",
    "ax[3].imshow(patch_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = torch.optim.Adam(model.parameters()) #adam is going to be the most robust, though perhaps not the best performing, typically a good place to start\n",
    "# optim = torch.optim.SGD(model.parameters(),\n",
    "#                           lr=.1,\n",
    "#                           momentum=0.9,\n",
    "#                           weight_decay=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "#we have the ability to weight individual classes, in this case we'll do so based on their presense in the trainingset\n",
    "#to avoid biasing any particular class\n",
    "nclasses = dataset[\"train\"].numpixels.shape[1]\n",
    "class_weight=dataset[\"train\"].numpixels[1,0:2] #don't take ignored class into account here\n",
    "class_weight = torch.from_numpy(1-class_weight/class_weight.sum()).type('torch.FloatTensor').to(device)\n",
    "\n",
    "print(class_weight) #show final used weights, make sure that they're reasonable before continouing\n",
    "criterion = nn.CrossEntropyLoss(weight = class_weight, ignore_index = ignore_index ,reduce=False) #reduce = False makes sure we get a 2D output instead of a 1D \"summary\" value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%load_ext line_profiler\n",
    "#%lprun\n",
    "# %%prun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "writer=SummaryWriter() #open the tensorboard visualiser\n",
    "best_loss_on_test = np.Infinity\n",
    "edge_weight=torch.tensor(edge_weight).to(device)\n",
    "start_time = time.time()\n",
    "for epoch in range(num_epochs):\n",
    "    #zero out epoch based performance variables \n",
    "    all_acc = {key: 0 for key in phases} \n",
    "    all_loss = {key: torch.zeros(0).to(device) for key in phases}\n",
    "    cmatrix = {key: np.zeros((2,2)) for key in phases}\n",
    "\n",
    "    for phase in phases: #iterate through both training and validation states\n",
    "\n",
    "        if phase == 'train':\n",
    "            model.train()  # Set model to training mode\n",
    "        else: #when in eval mode, we don't want parameters to be updated\n",
    "            model.eval()   # Set model to evaluate mode\n",
    "\n",
    "        for ii , (X, y, y_weight) in enumerate(dataLoader[phase]): #for each of the batches\n",
    "            X = X.to(device)  # [Nbatch, 3, H, W]\n",
    "            y_weight = y_weight.type('torch.FloatTensor').to(device)\n",
    "            y = y.type('torch.LongTensor').to(device)  # [Nbatch, H, W] with class indices (0, 1)\n",
    "\n",
    "            with torch.set_grad_enabled(phase == 'train'): #dynamically set gradient computation, in case of validation, this isn't needed\n",
    "                                                            #disabling is good practice and improves inference time\n",
    "\n",
    "                prediction = model(X)  # [N, Nclass, H, W]\n",
    "                loss_matrix = criterion(prediction, y)\n",
    "                loss = (loss_matrix * (edge_weight**y_weight)).mean() #can skip if edge weight==1\n",
    "\n",
    "                if phase==\"train\": #in case we're in train mode, need to do back propogation\n",
    "                    optim.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optim.step()\n",
    "                    train_loss = loss\n",
    "\n",
    "\n",
    "                all_loss[phase]=torch.cat((all_loss[phase],loss.detach().view(1,-1)))\n",
    "\n",
    "                if phase in validation_phases: #if this phase is part of validation, compute confusion matrix\n",
    "                    p=prediction[:,:,:,:].detach().cpu().numpy()\n",
    "                    cpredflat=np.argmax(p,axis=1).flatten()\n",
    "                    yflat=y.cpu().numpy().flatten()\n",
    "\n",
    "                    cmatrix[phase]=cmatrix[phase]+confusion_matrix(yflat,cpredflat,labels=range(n_classes))\n",
    "\n",
    "        all_acc[phase]=(cmatrix[phase]/cmatrix[phase].sum()).trace()\n",
    "        all_loss[phase] = all_loss[phase].cpu().numpy().mean()\n",
    "\n",
    "        #save metrics to tensorboard\n",
    "        writer.add_scalar(f'{phase}/loss', all_loss[phase], epoch)\n",
    "        if phase in validation_phases:\n",
    "            writer.add_scalar(f'{phase}/acc', all_acc[phase], epoch)\n",
    "            writer.add_scalar(f'{phase}/TN', cmatrix[phase][0,0], epoch)\n",
    "            writer.add_scalar(f'{phase}/TP', cmatrix[phase][1,1], epoch)\n",
    "            writer.add_scalar(f'{phase}/FP', cmatrix[phase][0,1], epoch)\n",
    "            writer.add_scalar(f'{phase}/FN', cmatrix[phase][1,0], epoch)\n",
    "            writer.add_scalar(f'{phase}/TNR', cmatrix[phase][0,0]/(cmatrix[phase][0,0]+cmatrix[phase][0,1]), epoch)\n",
    "            writer.add_scalar(f'{phase}/TPR', cmatrix[phase][1,1]/(cmatrix[phase][1,1]+cmatrix[phase][1,0]), epoch)\n",
    "\n",
    "    print('%s ([%d/%d] %d%%), train loss: %.4f test loss: %.4f' % (timeSince(start_time, (epoch+1) / num_epochs), \n",
    "                                                 epoch+1, num_epochs ,(epoch+1) / num_epochs * 100, all_loss[\"train\"], all_loss[\"val\"]),end=\"\")    \n",
    "\n",
    "    #if current loss is the best we've seen, save model state with all variables\n",
    "    #necessary for recreation\n",
    "    if all_loss[\"val\"] < best_loss_on_test:\n",
    "        best_loss_on_test = all_loss[\"val\"]\n",
    "        print(\"  **\")\n",
    "        state = {'epoch': epoch + 1,\n",
    "         'model_dict': model.state_dict(),\n",
    "         'optim_dict': optim.state_dict(),\n",
    "         'best_loss_on_test': all_loss,\n",
    "         'n_classes': n_classes,\n",
    "         'in_channels': in_channels,\n",
    "         'padding': padding,\n",
    "         'depth': depth,\n",
    "         'wf': wf,\n",
    "         'up_mode': up_mode, 'batch_norm': batch_norm}\n",
    "\n",
    "\n",
    "        torch.save(state, f\"{dataname}_unet_best_model.pth\")\n",
    "    else:\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%lprun -f trainnetwork trainnetwork(edge_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#At this stage, training is done...below are snippets to help with other tasks: output generation + visualization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----- generate output\n",
    "#load best model\n",
    "checkpoint = torch.load(f\"{dataname}_unet_best_model.pth\")\n",
    "model.load_state_dict(checkpoint[\"model_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grab a single image from validation set\n",
    "[img,mask,mask_weight]=dataset[\"val\"][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate its output\n",
    "#%%timeit\n",
    "output=model(img[None,::].to(device))\n",
    "output=output.detach().squeeze().cpu().numpy()\n",
    "output=np.moveaxis(output,0,-1) \n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize its result\n",
    "fig, ax = plt.subplots(1,4, figsize=(10,4))  # 1 row, 2 columns\n",
    "\n",
    "ax[0].imshow(output[:,:,1])\n",
    "ax[1].imshow(np.argmax(output,axis=2))\n",
    "ax[2].imshow(mask)\n",
    "ax[3].imshow(np.moveaxis(img.numpy(),0,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------- visualize kernels and activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper function for visualization\n",
    "def plot_kernels(tensor, num_cols=8 ,cmap=\"gray\"):\n",
    "    if not len(tensor.shape)==4:\n",
    "        raise Exception(\"assumes a 4D tensor\")\n",
    "#    if not tensor.shape[1]==3:\n",
    "#        raise Exception(\"last dim needs to be 3 to plot\")\n",
    "    num_kernels = tensor.shape[0] * tensor.shape[1]\n",
    "    num_rows = 1+ num_kernels // num_cols\n",
    "    fig = plt.figure(figsize=(num_cols,num_rows))\n",
    "    i=0\n",
    "    t=tensor.data.numpy()\n",
    "    for t1 in t:\n",
    "        for t2 in t1:\n",
    "            i+=1\n",
    "            ax1 = fig.add_subplot(num_rows,num_cols,i)\n",
    "            ax1.imshow(t2 , cmap=cmap)\n",
    "            ax1.axis('off')\n",
    "            ax1.set_xticklabels([])\n",
    "            ax1.set_yticklabels([])\n",
    "\n",
    "    plt.subplots_adjust(wspace=0.1, hspace=0.1)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerActivations():\n",
    "    features=None\n",
    "    def __init__(self,layer):\n",
    "        self.hook = layer.register_forward_hook(self.hook_fn)\n",
    "    def hook_fn(self,module,input,output):\n",
    "        self.features = output.cpu()\n",
    "    def remove(self):\n",
    "        self.hook.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- visualize kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w=model.up_path[2].conv_block.block[3]\n",
    "plot_kernels(w.weight.detach().cpu(),8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- visualize activiations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dr=LayerActivations(model.up_path[2].conv_block.block[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[img,mask,mask_weight]=dataset[\"val\"][7]\n",
    "plt.imshow(np.moveaxis(img.numpy(),0,-1))\n",
    "output=model(img[None,::].to(device))\n",
    "plot_kernels(dr.features,8,cmap=\"rainbow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "# ---- Improvements:\n",
    "1 replace Adam with SGD with appropriate learning rate reduction"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
