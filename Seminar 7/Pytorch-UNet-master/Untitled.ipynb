{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Using device cuda\n",
      "INFO: Network:\n",
      "\t3 input channels\n",
      "\t1 output channels (classes)\n",
      "\tBilinear upscaling\n",
      "Epoch 1/5: 0img [00:00, ?img/s]\n",
      "INFO: Checkpoint 1 saved !\n",
      "Validation round: 0img [00:00, ?img/s]\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-01e780e397c7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m     train_net(net=net,\n\u001b[1;32m--> 137\u001b[1;33m               device=device)\n\u001b[0m\u001b[0;32m    138\u001b[0m \u001b[1;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'INTERRUPTED.pth'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-11-01e780e397c7>\u001b[0m in \u001b[0;36mtrain_net\u001b[1;34m(net, device, epochs, batch_size, lr, val_percent, save_cp, img_scale)\u001b[0m\n\u001b[0;32m     78\u001b[0m             \u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'Checkpoint {epoch + 1} saved !'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 80\u001b[1;33m         \u001b[0mval_dice\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0meval_net\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     81\u001b[0m         \u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Validation Dice Coeff: {}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_dice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\OneDrive - C\\techno2\\DM2\\my_nn\\Seminar 7\\Pytorch-UNet-master\\eval.py\u001b[0m in \u001b[0;36meval_net\u001b[1;34m(net, dataset, device, n_val)\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[0mmask_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mmask_pred\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[0mtot\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mdice_coeff\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmask_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrue_mask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtot\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mn_val\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "from eval import eval_net\n",
    "from unet import UNet\n",
    "from utils import get_ids, split_train_val, get_imgs_and_masks, batch\n",
    "\n",
    "dir_img = 'data/imgs/'\n",
    "dir_mask = 'data/masks/'\n",
    "dir_checkpoint = 'checkpoints/'\n",
    "\n",
    "\n",
    "def train_net(net,\n",
    "              device,\n",
    "              epochs=5,\n",
    "              batch_size=1,\n",
    "              lr=0.1,\n",
    "              val_percent=0.15,\n",
    "              save_cp=True,\n",
    "              img_scale=0.5):\n",
    "    ids = get_ids(dir_img)\n",
    "\n",
    "    iddataset = split_train_val(ids, val_percent)\n",
    "\n",
    "\n",
    "    n_train = len(iddataset['train'])\n",
    "    n_val = len(iddataset['val'])\n",
    "    optimizer = optim.Adam(net.parameters(), lr=lr)\n",
    "    criterion = nn.BCELoss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        net.train()\n",
    "\n",
    "        # reset the generators\n",
    "        train = get_imgs_and_masks(iddataset['train'], dir_img, dir_mask, img_scale)\n",
    "        val = get_imgs_and_masks(iddataset['val'], dir_img, dir_mask, img_scale)\n",
    "\n",
    "        epoch_loss = 0\n",
    "        with tqdm(total=n_train, desc=f'Epoch {epoch + 1}/{epochs}', unit='img') as pbar:\n",
    "            for i, b in enumerate(batch(train, batch_size)):\n",
    "                imgs = np.array([i[0] for i in b]).astype(np.float32)\n",
    "                true_masks = np.array([i[1] for i in b])\n",
    "\n",
    "                imgs = torch.from_numpy(imgs)\n",
    "                true_masks = torch.from_numpy(true_masks)\n",
    "\n",
    "                imgs = imgs.to(device=device)\n",
    "                true_masks = true_masks.to(device=device)\n",
    "\n",
    "                masks_pred = net(imgs)\n",
    "                loss = criterion(masks_pred, true_masks)\n",
    "                epoch_loss += loss.item()\n",
    "\n",
    "                pbar.set_postfix(**{'loss (batch)': loss.item()})\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                pbar.update(batch_size)\n",
    "\n",
    "        if save_cp:\n",
    "            try:\n",
    "                os.mkdir(dir_checkpoint)\n",
    "                logging.info('Created checkpoint directory')\n",
    "            except OSError:\n",
    "                pass\n",
    "            torch.save(net.state_dict(),\n",
    "                       dir_checkpoint + f'CP_epoch{epoch + 1}.pth')\n",
    "            logging.info(f'Checkpoint {epoch + 1} saved !')\n",
    "\n",
    "        val_dice = eval_net(net, val, device, n_val)\n",
    "        logging.info('Validation Dice Coeff: {}'.format(val_dice))\n",
    "\n",
    "\n",
    "def get_args():\n",
    "    parser = argparse.ArgumentParser(description='Train the UNet on images and target masks',\n",
    "                                     formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
    "    parser.add_argument('-e', '--epochs', metavar='E', type=int, default=5,\n",
    "                        help='Number of epochs', dest='epochs')\n",
    "    parser.add_argument('-b', '--batch-size', metavar='B', type=int, nargs='?', default=1,\n",
    "                        help='Batch size', dest='batchsize')\n",
    "    parser.add_argument('-l', '--learning-rate', metavar='LR', type=float, nargs='?', default=0.1,\n",
    "                        help='Learning rate', dest='lr')\n",
    "    parser.add_argument('-f', '--load', dest='load', type=str, default=False,\n",
    "                        help='Load model from a .pth file')\n",
    "    parser.add_argument('-s', '--scale', dest='scale', type=float, default=0.5,\n",
    "                        help='Downscaling factor of the images')\n",
    "    parser.add_argument('-v', '--validation', dest='val', type=float, default=15.0,\n",
    "                        help='Percent of the data that is used as validation (0-100)')\n",
    "\n",
    "    return parser.parse_args()\n",
    "\n",
    "\n",
    "def pretrain_checks():\n",
    "    imgs = [f for f in os.listdir(dir_img) if not f.startswith('.')]\n",
    "    masks = [f for f in os.listdir(dir_mask) if not f.startswith('.')]\n",
    "    if len(imgs) != len(masks):\n",
    "        logging.warning(f'The number of images and masks do not match ! '\n",
    "                        f'{len(imgs)} images and {len(masks)} masks detected in the data folder.')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')\n",
    "    args = get_args()\n",
    "    pretrain_checks()\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    logging.info(f'Using device {device}')\n",
    "\n",
    "    # Change here to adapt to your data\n",
    "    # n_channels=3 for RGB images\n",
    "    # n_classes is the number of probabilities you want to get per pixel\n",
    "    #   - For 1 class and background, use n_classes=1\n",
    "    #   - For 2 classes, use n_classes=1\n",
    "    #   - For N > 2 classes, use n_classes=N\n",
    "    net = UNet(n_channels=3, n_classes=1)\n",
    "    logging.info(f'Network:\\n'\n",
    "                 f'\\t{net.n_channels} input channels\\n'\n",
    "                 f'\\t{net.n_classes} output channels (classes)\\n'\n",
    "                 f'\\t{\"Bilinear\" if net.bilinear else \"Dilated conv\"} upscaling')\n",
    "\n",
    "\n",
    "    net.to(device=device)\n",
    "    # faster convolutions, but more memory\n",
    "    # cudnn.benchmark = True\n",
    "\n",
    "try:\n",
    "    train_net(net=net,\n",
    "              device=device)\n",
    "except KeyboardInterrupt:\n",
    "    torch.save(net.state_dict(), 'INTERRUPTED.pth')\n",
    "    logging.info('Saved interrupt')\n",
    "    try:\n",
    "        sys.exit(0)\n",
    "    except SystemExit:\n",
    "        os._exit(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
