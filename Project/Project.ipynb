{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import time\n",
    "import argparse\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Our Modules\n",
    "import datasets\n",
    "from datasets import utils\n",
    "from models.MPNN import MPNN\n",
    "from models.MPNN_Duvenaud import MpnnDuvenaud\n",
    "from models.MPNN_GGNN import MpnnGGNN\n",
    "from models.MPNN_IntNet import MpnnIntNet\n",
    "from LogMetric import AverageMeter, Logger\n",
    "\n",
    "from LogMetric import AverageMeter, Logger\n",
    "\n",
    "def restricted_float(x, inter):\n",
    "    x = float(x)\n",
    "    if x < inter[0] or x > inter[1]:\n",
    "        raise argparse.ArgumentTypeError(\"%r not in range [1e-5, 1e-4]\"%(x,))\n",
    "    return x\n",
    "\n",
    "# Argument parser\n",
    "parser = argparse.ArgumentParser(description='Neural message passing')\n",
    "\n",
    "parser.add_argument('--dataset', default='our_data')\n",
    "parser.add_argument('--datasetPath', default='./data-parse/chemfindata/', help='dataset path')\n",
    "parser.add_argument('--logPath', default='./log/our_data/mpnn/', help='log path')\n",
    "parser.add_argument('--plotLr', default=False, help='allow plotting the data')\n",
    "parser.add_argument('--plotPath', default='./plot/our_data/mpnn/', help='plot path')\n",
    "parser.add_argument('--resume', default='./checkpoint/our_data/mpnn/',\n",
    "                    help='path to latest checkpoint')\n",
    "# Optimization Options\n",
    "parser.add_argument('--batch-size', type=int, default=100, metavar='N',\n",
    "                    help='Input batch size for training (default: 20)')\n",
    "parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "                    help='Enables CUDA training')\n",
    "parser.add_argument('--epochs', type=int, default=360, metavar='N',\n",
    "                    help='Number of epochs to train (default: 360)')\n",
    "parser.add_argument('--lr', type=lambda x: restricted_float(x, [1e-5, 1e-2]), default=1e-3, metavar='LR',\n",
    "                    help='Initial learning rate [1e-5, 5e-4] (default: 1e-4)')\n",
    "parser.add_argument('--lr-decay', type=lambda x: restricted_float(x, [.01, 1]), default=0.6, metavar='LR-DECAY',\n",
    "                    help='Learning rate decay factor [.01, 1] (default: 0.6)')\n",
    "parser.add_argument('--schedule', type=list, default=[0.1, 0.9], metavar='S',\n",
    "                    help='Percentage of epochs to start the learning rate decay [0, 1] (default: [0.1, 0.9])')\n",
    "parser.add_argument('--momentum', type=float, default=0.9, metavar='M',\n",
    "                    help='SGD momentum (default: 0.9)')\n",
    "# i/o\n",
    "parser.add_argument('--log-interval', type=int, default=20, metavar='N',\n",
    "                    help='How many batches to wait before logging training status')\n",
    "# Accelerating\n",
    "parser.add_argument('--prefetch', type=int, default=2, help='Pre-fetching threads.')\n",
    "\n",
    "best_er1 = 0\n",
    "\n",
    "\n",
    "\n",
    "args = parser.parse_args([])\n",
    "\n",
    "args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "\n",
    "# Load data\n",
    "root = args.datasetPath\n",
    "\n",
    "print('Prepare files')\n",
    "files = [f for f in os.listdir(root) if os.path.isfile(os.path.join(root, f))]\n",
    "\n",
    "idx = np.random.permutation(len(files))\n",
    "idx = idx.tolist()\n",
    "\n",
    "valid_ids = [files[i] for i in idx[0:10000]]\n",
    "test_ids = [files[i] for i in idx[10000:20000]]\n",
    "train_ids = [files[i] for i in idx[20000:]]\n",
    "\n",
    "data_train = datasets.Qm9(root, train_ids, edge_transform=utils.qm9_edges, e_representation='raw_distance')\n",
    "data_valid = datasets.Qm9(root, valid_ids, edge_transform=utils.qm9_edges, e_representation='raw_distance')\n",
    "data_test = datasets.Qm9(root, test_ids, edge_transform=utils.qm9_edges, e_representation='raw_distance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def restricted_float(x, inter):\n",
    "    x = float(x)\n",
    "    if x < inter[0] or x > inter[1]:\n",
    "        raise argparse.ArgumentTypeError(\"%r not in range [1e-5, 1e-4]\"%(x,))\n",
    "    return x\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Neural message passing')\n",
    "\n",
    "parser.add_argument('--dataset', default='our_data')\n",
    "parser.add_argument('--datasetPath', default='./data-parse/chemfindata/', help='dataset path')\n",
    "parser.add_argument('--logPath', default='./log/our_data/mpnn/', help='log path')\n",
    "parser.add_argument('--plotLr', default=False, help='allow plotting the data')\n",
    "parser.add_argument('--plotPath', default='./plot/our_data/mpnn/', help='plot path')\n",
    "parser.add_argument('--resume', default='./checkpoint/our_data/mpnn/',\n",
    "                    help='path to latest checkpoint')\n",
    "# Optimization Options\n",
    "parser.add_argument('--batch-size', type=int, default=100, metavar='N',\n",
    "                    help='Input batch size for training (default: 20)')\n",
    "parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "                    help='Enables CUDA training')\n",
    "parser.add_argument('--epochs', type=int, default=360, metavar='N',\n",
    "                    help='Number of epochs to train (default: 360)')\n",
    "parser.add_argument('--lr', type=lambda x: restricted_float(x, [1e-5, 1e-2]), default=1e-3, metavar='LR',\n",
    "                    help='Initial learning rate [1e-5, 5e-4] (default: 1e-4)')\n",
    "parser.add_argument('--lr-decay', type=lambda x: restricted_float(x, [.01, 1]), default=0.6, metavar='LR-DECAY',\n",
    "                    help='Learning rate decay factor [.01, 1] (default: 0.6)')\n",
    "parser.add_argument('--schedule', type=list, default=[0.1, 0.9], metavar='S',\n",
    "                    help='Percentage of epochs to start the learning rate decay [0, 1] (default: [0.1, 0.9])')\n",
    "parser.add_argument('--momentum', type=float, default=0.9, metavar='M',\n",
    "                    help='SGD momentum (default: 0.9)')\n",
    "# i/o\n",
    "parser.add_argument('--log-interval', type=int, default=20, metavar='N',\n",
    "                    help='How many batches to wait before logging training status')\n",
    "# Accelerating\n",
    "parser.add_argument('--prefetch', type=int, default=2, help='Pre-fetching threads.')\n",
    "\n",
    "best_er1 = 0\n",
    "reader_folder = os.path.realpath( os.path.abspath('../GraphReader'))\n",
    "if reader_folder not in sys.path:\n",
    "    sys.path.insert(1, reader_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_graph(prop):\n",
    "    \n",
    "    prop = prop.split()\n",
    "    g_tag = prop[0]\n",
    "    g_index = int(prop[1])\n",
    "    g_A = float(prop[2])\n",
    "    g_B = float(prop[3]) \n",
    "    g_C = float(prop[4]) \n",
    "    g_mu = float(prop[5])\n",
    "    g_alpha = float(prop[6]) \n",
    "    g_homo = float(prop[7])\n",
    "    g_lumo = float(prop[8]) \n",
    "    g_gap = float(prop[9])\n",
    "    g_r2 = float(prop[10])\n",
    "    g_zpve = float(prop[11]) \n",
    "    g_U0 = float(prop[12]) \n",
    "    g_U = float(prop[13])\n",
    "    g_H = float(prop[14])\n",
    "    g_G = float(prop[15])\n",
    "    g_Cv = float(prop[16])\n",
    "\n",
    "    labels = [g_mu, g_alpha, g_homo, g_lumo, g_gap, g_r2, g_zpve, g_U0, g_U, g_H, g_G, g_Cv]\n",
    "    return nx.Graph(tag=g_tag, index=g_index, A=g_A, B=g_B, C=g_C, mu=g_mu, alpha=g_alpha, homo=g_homo,\n",
    "                    lumo=g_lumo, gap=g_gap, r2=g_r2, zpve=g_zpve, U0=g_U0, U=g_U, H=g_H, G=g_G, Cv=g_Cv), labels\n",
    "\n",
    "def init_graph_our(g_tag,prop):\n",
    "    \n",
    "    prop = prop.split()\n",
    "    g_H=float(prop[0])\n",
    "    g_p=float(prop[1])\n",
    "\n",
    "    labels = [g_H, g_p]\n",
    "    return nx.Graph(tag=g_tag,H=g_H, p=g_p), labels\n",
    "\n",
    "\n",
    "# XYZ file reader for QM9 dataset\n",
    "def xyz_graph_reader(graph_file):\n",
    "\n",
    "    with open(graph_file,'r') as f:\n",
    "        # Number of atoms\n",
    "        #na = int(f.readline())\n",
    "        tag=f.readline().strip()\n",
    "        # Graph properties\n",
    "        properties = f.readline()\n",
    "        na=int(f.readline())\n",
    "        #g, l = init_graph(properties)\n",
    "        g,l=init_graph_our(tag, properties)\n",
    "        atom_properties = []\n",
    "        # Atoms properties\n",
    "        for i in range(na):\n",
    "            a_properties = f.readline()\n",
    "            a_properties = a_properties.replace('.*^', 'e')\n",
    "            a_properties = a_properties.replace('*^', 'e')\n",
    "            a_properties = a_properties.split()\n",
    "            atom_properties.append(a_properties)\n",
    "\n",
    "        # Frequencies\n",
    "        #f.readline()\n",
    "\n",
    "        # SMILES\n",
    "        smiles = f.readline()\n",
    "        smiles = smiles.split()\n",
    "        smiles = smiles[0]\n",
    "        \n",
    "        m = Chem.MolFromSmiles(smiles)\n",
    "        m = Chem.AddHs(m)\n",
    "        \n",
    "        fdef_name = os.path.join(RDConfig.RDDataDir, 'BaseFeatures.fdef')\n",
    "        factory = ChemicalFeatures.BuildFeatureFactory(fdef_name)\n",
    "        feats = factory.GetFeaturesForMol(m)\n",
    "\n",
    "        # Create nodes\n",
    "        for i in range(0, m.GetNumAtoms()):\n",
    "            atom_i = m.GetAtomWithIdx(i)\n",
    "\n",
    "            g.add_node(i, a_type=atom_i.GetSymbol(), a_num=atom_i.GetAtomicNum(), acceptor=0, donor=0,\n",
    "                       aromatic=atom_i.GetIsAromatic(), hybridization=atom_i.GetHybridization(),\n",
    "                       num_h=atom_i.GetTotalNumHs(), coord=np.array(atom_properties[i][1:4]).astype(np.float),\n",
    "                       pc=float(atom_properties[i][4]))\n",
    "\n",
    "        for i in range(0, len(feats)):\n",
    "            if feats[i].GetFamily() == 'Donor':\n",
    "                node_list = feats[i].GetAtomIds()\n",
    "                for i in node_list:\n",
    "                    g.node[i]['donor'] = 1\n",
    "            elif feats[i].GetFamily() == 'Acceptor':\n",
    "                node_list = feats[i].GetAtomIds()\n",
    "                for i in node_list:\n",
    "                    g.node[i]['acceptor'] = 1\n",
    "\n",
    "        # Read Edges\n",
    "        for i in range(0, m.GetNumAtoms()):\n",
    "            for j in range(0, m.GetNumAtoms()):\n",
    "                e_ij = m.GetBondBetweenAtoms(i, j)\n",
    "                if e_ij is not None:\n",
    "                    g.add_edge(i, j, b_type=e_ij.GetBondType(),\n",
    "                               distance=np.linalg.norm(g.node[i]['coord']-g.node[j]['coord']))\n",
    "                else:\n",
    "                    # Unbonded\n",
    "                    g.add_edge(i, j, b_type=None,\n",
    "                               distance=np.linalg.norm(g.node[i]['coord'] - g.node[j]['coord']))\n",
    "    return g , l\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import rdkit\n",
    "import torch\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def qm9_nodes(g, hydrogen=False):\n",
    "    h = []\n",
    "    for n, d in list(g.nodes(data=True)):\n",
    "        h_t = []\n",
    "        # Atom type (One-hot H, C, N, O F)\n",
    "        h_t += [int(d['a_type'] == x) for x in ['H', 'C', 'N', 'O', 'F']]\n",
    "        # Atomic number\n",
    "        h_t.append(d['a_num'])\n",
    "        # Partial Charge\n",
    "        h_t.append(d['pc'])\n",
    "        # Acceptor\n",
    "        h_t.append(d['acceptor'])\n",
    "        # Donor\n",
    "        h_t.append(d['donor'])\n",
    "        # Aromatic\n",
    "        h_t.append(int(d['aromatic']))\n",
    "        # Hybradization\n",
    "        h_t += [int(d['hybridization'] == x) for x in [rdkit.Chem.rdchem.HybridizationType.SP, rdkit.Chem.rdchem.HybridizationType.SP2, rdkit.Chem.rdchem.HybridizationType.SP3]]\n",
    "        # If number hydrogen is used as a\n",
    "        if hydrogen:\n",
    "            h_t.append(d['num_h'])\n",
    "        h.append(h_t)\n",
    "    return h\n",
    "\n",
    "\n",
    "def qm9_edges(g, e_representation='raw_distance'):\n",
    "    remove_edges = []\n",
    "    e={}    \n",
    "    for n1, n2, d in g.edges_iter(data=True):\n",
    "        e_t = []\n",
    "        # Raw distance function\n",
    "        if e_representation == 'chem_graph':\n",
    "            if d['b_type'] is None:\n",
    "                remove_edges += [(n1, n2)]\n",
    "            else:\n",
    "                e_t += [i+1 for i, x in enumerate([rdkit.Chem.rdchem.BondType.SINGLE, rdkit.Chem.rdchem.BondType.DOUBLE,\n",
    "                                                rdkit.Chem.rdchem.BondType.TRIPLE, rdkit.Chem.rdchem.BondType.AROMATIC])\n",
    "                        if x == d['b_type']]\n",
    "        elif e_representation == 'distance_bin':\n",
    "            if d['b_type'] is None:\n",
    "                step = (6-2)/8.0\n",
    "                start = 2\n",
    "                b = 9\n",
    "                for i in range(0, 9):\n",
    "                    if d['distance'] < (start+i*step):\n",
    "                        b = i\n",
    "                        break\n",
    "                e_t.append(b+5)\n",
    "            else:\n",
    "                e_t += [i+1 for i, x in enumerate([rdkit.Chem.rdchem.BondType.SINGLE, rdkit.Chem.rdchem.BondType.DOUBLE,\n",
    "                                                   rdkit.Chem.rdchem.BondType.TRIPLE, rdkit.Chem.rdchem.BondType.AROMATIC])\n",
    "                        if x == d['b_type']]\n",
    "        elif e_representation == 'raw_distance':\n",
    "            if d['b_type'] is None:\n",
    "                remove_edges += [(n1, n2)]\n",
    "            else:\n",
    "                e_t.append(d['distance'])\n",
    "                e_t += [int(d['b_type'] == x) for x in [rdkit.Chem.rdchem.BondType.SINGLE, rdkit.Chem.rdchem.BondType.DOUBLE,\n",
    "                                                        rdkit.Chem.rdchem.BondType.TRIPLE, rdkit.Chem.rdchem.BondType.AROMATIC]]\n",
    "        else:\n",
    "            print('Incorrect Edge representation transform')\n",
    "            quit()\n",
    "        if e_t:\n",
    "            e[(n1, n2)] = e_t\n",
    "    for edg in remove_edges:\n",
    "        g.remove_edge(*edg)\n",
    "    return nx.to_numpy_matrix(g), e\n",
    "    \n",
    "\n",
    "def normalize_data(data, mean, std):\n",
    "    data_norm = (data-mean)/std\n",
    "    return data_norm\n",
    "\n",
    "\n",
    "def get_values(obj, start, end, prop):\n",
    "    vals = []\n",
    "    for i in range(start, end):\n",
    "        v = {}\n",
    "        if 'degrees' in prop:\n",
    "            v['degrees'] = set(sum(obj[i][0][0].sum(axis=0, dtype='int').tolist(), []))\n",
    "        if 'edge_labels' in prop:\n",
    "            v['edge_labels'] = set(sum(list(obj[i][0][2].values()), []))\n",
    "        if 'target_mean' in prop or 'target_std' in prop:\n",
    "            v['params'] = obj[i][1]\n",
    "        vals.append(v)\n",
    "    return vals\n",
    "\n",
    "\n",
    "def get_graph_stats(graph_obj_handle, prop='degrees'):\n",
    "    # if prop == 'degrees':\n",
    "    num_cores = multiprocessing.cpu_count()\n",
    "    inputs = [int(i*len(graph_obj_handle)/num_cores) for i in range(num_cores)] + [len(graph_obj_handle)]\n",
    "    res = Parallel(n_jobs=num_cores)(delayed(get_values)(graph_obj_handle, inputs[i], inputs[i+1], prop) for i in range(num_cores))\n",
    "\n",
    "    stat_dict = {}\n",
    "\n",
    "    if 'degrees' in prop:\n",
    "        stat_dict['degrees'] = list(set([d for core_res in res for file_res in core_res for d in file_res['degrees']]))\n",
    "    if 'edge_labels' in prop:\n",
    "        stat_dict['edge_labels'] = list(set([d for core_res in res for file_res in core_res for d in file_res['edge_labels']]))\n",
    "    if 'target_mean' in prop or 'target_std' in prop:\n",
    "        param = np.array([file_res['params'] for core_res in res for file_res in core_res])\n",
    "    if 'target_mean' in prop:\n",
    "        stat_dict['target_mean'] = np.mean(param, axis=0)\n",
    "    if 'target_std' in prop:\n",
    "        stat_dict['target_std'] = np.std(param, axis=0)\n",
    "\n",
    "    return stat_dict\n",
    "\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    pred = pred.type_as(target)\n",
    "    target = target.type_as(pred)\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].view(-1).float().sum(0)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res\n",
    "\n",
    "\n",
    "def collate_g(batch):\n",
    "\n",
    "    batch_sizes = np.max(np.array([[len(input_b[1]), len(input_b[1][0]), len(input_b[2]),\n",
    "                                len(list(input_b[2].values())[0])]\n",
    "                                if input_b[2] else\n",
    "                                [len(input_b[1]), len(input_b[1][0]), 0,0]\n",
    "                                for (input_b, target_b) in batch]), axis=0)\n",
    "\n",
    "    g = np.zeros((len(batch), batch_sizes[0], batch_sizes[0]))\n",
    "    h = np.zeros((len(batch), batch_sizes[0], batch_sizes[1]))\n",
    "    e = np.zeros((len(batch), batch_sizes[0], batch_sizes[0], batch_sizes[3]))\n",
    "\n",
    "    target = np.zeros((len(batch), len(batch[0][1])))\n",
    "\n",
    "    for i in range(len(batch)):\n",
    "\n",
    "        num_nodes = len(batch[i][0][1])\n",
    "\n",
    "        # Adjacency matrix\n",
    "        g[i, 0:num_nodes, 0:num_nodes] = batch[i][0][0]\n",
    "\n",
    "        # Node features\n",
    "        h[i, 0:num_nodes, :] = batch[i][0][1]\n",
    "\n",
    "        # Edges\n",
    "        for edge in batch[i][0][2].keys():\n",
    "            e[i, edge[0], edge[1], :] = batch[i][0][2][edge]\n",
    "            e[i, edge[1], edge[0], :] = batch[i][0][2][edge]\n",
    "\n",
    "        # Target\n",
    "        target[i, :] = batch[i][1]\n",
    "\n",
    "    g = torch.FloatTensor(g)\n",
    "    h = torch.FloatTensor(h)\n",
    "    e = torch.FloatTensor(e)\n",
    "    target = torch.FloatTensor(target)\n",
    "\n",
    "    return g, h, e, target\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Qm9(data.Dataset):\n",
    "\n",
    "    # Constructor\n",
    "    def __init__(self, root_path, ids, vertex_transform=utils.qm9_nodes, edge_transform=utils.qm9_edges,\n",
    "                 target_transform=None, e_representation='raw_distance'):\n",
    "        self.root = root_path\n",
    "        self.ids = ids\n",
    "        self.vertex_transform = vertex_transform\n",
    "        self.edge_transform = edge_transform\n",
    "        self.target_transform = target_transform\n",
    "        self.e_representation = e_representation\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        g, target = xyz_graph_reader(os.path.join(self.root, self.ids[index]))\n",
    "        if self.vertex_transform is not None:\n",
    "            h = self.vertex_transform(g)\n",
    "\n",
    "        if self.edge_transform is not None:\n",
    "            g, e = qm9_edges(g, self.e_representation)\n",
    "\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "\n",
    "        return (g, h, e), target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "\n",
    "    def set_target_transform(self, target_transform):\n",
    "        self.target_transform = target_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1346\n",
      "1000\n",
      "1000\n",
      "((matrix([[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0.,\n",
      "         0., 0., 0., 0.],\n",
      "        [1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0.],\n",
      "        [0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
      "         0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "         0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 1., 1.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0.]]), [[0, 1, 0, 0, 0, 6, -0.52, 0, 0, 0, 0, 0, 1], [0, 1, 0, 0, 0, 6, -0.52, 0, 0, 1, 0, 1, 0], [0, 1, 0, 0, 0, 6, 0.91, 0, 0, 1, 0, 1, 0], [0, 1, 0, 0, 0, 6, 0.13, 0, 0, 1, 0, 1, 0], [0, 1, 0, 0, 0, 6, -0.14, 0, 0, 1, 0, 1, 0], [0, 1, 0, 0, 0, 6, -0.14, 0, 0, 1, 0, 1, 0], [0, 1, 0, 0, 0, 6, -0.15, 0, 0, 1, 0, 1, 0], [0, 1, 0, 0, 0, 6, -0.15, 0, 0, 0, 0, 0, 1], [0, 0, 1, 0, 0, 7, -0.15, 0, 0, 0, 0, 1, 0], [0, 0, 0, 1, 0, 8, 0.14, 0, 0, 0, 0, 1, 0], [0, 0, 0, 1, 0, 8, 0.14, 0, 0, 0, 0, 1, 0], [1, 0, 0, 0, 0, 1, 0.15, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 1, 0.15, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 1, 0.15, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 1, 0.0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 1, 0.0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 1, 0.0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 1, 0.0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 1, 0.0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 1, 0.0, 0, 0, 0, 0, 0, 0]], {(0, 1): [2.1952000022776965, 1, 0, 0, 0], (0, 11): [4.7958660802820585, 1, 0, 0, 0], (0, 12): [5.691253728309782, 1, 0, 0, 0], (0, 13): [6.004944686672809, 1, 0, 0, 0], (1, 2): [1.2524762073588462, 0, 0, 0, 1], (1, 6): [4.721597983733897, 0, 0, 0, 1], (2, 3): [1.4247999999999998, 0, 0, 0, 1], (2, 8): [4.2146, 1, 0, 0, 0], (3, 4): [1.3948586881831435, 0, 0, 0, 1], (3, 7): [2.415984952353801, 1, 0, 0, 0], (4, 5): [2.416, 0, 0, 0, 1], (4, 14): [2.161059659056177, 1, 0, 0, 0], (5, 6): [2.7897673756067904, 0, 0, 0, 1], (5, 15): [4.193503263382539, 1, 0, 0, 0], (6, 16): [2.545176687383412, 1, 0, 0, 0], (7, 17): [3.2007759840388705, 1, 0, 0, 0], (7, 18): [3.2008675105352298, 1, 0, 0, 0], (7, 19): [2.5451223664099136, 1, 0, 0, 0], (8, 9): [3.77286609621916, 0, 1, 0, 0], (8, 10): [3.772791705090542, 1, 0, 0, 0]}), [48.6341, 1.282])\n",
      "\n",
      "\n",
      "((matrix([[0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1.,\n",
      "         0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 1., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 1., 1., 1.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0.]]), [[0, 1, 0, 0, 0, 6, -0.57, 0, 0, 0, 0, 0, 1], [0, 1, 0, 0, 0, 6, 0.06, 0, 0, 0, 0, 0, 1], [0, 1, 0, 0, 0, 6, 0.45, 0, 0, 0, 0, 1, 0], [0, 0, 0, 1, 0, 8, 0.0, 1, 0, 0, 0, 1, 0], [0, 1, 0, 0, 0, 6, 0.0, 0, 0, 0, 0, 0, 1], [0, 1, 0, 0, 0, 6, 0.0, 0, 0, 0, 0, 0, 1], [0, 1, 0, 0, 0, 6, 0.06, 0, 0, 0, 0, 0, 1], [0, 1, 0, 0, 0, 6, 0.0, 0, 0, 0, 0, 0, 1], [1, 0, 0, 0, 0, 1, 0.0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 1, 0.0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 1, 0.0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 1, 0.0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 1, 0.0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 1, 0.0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 1, 0.0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 1, 0.0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 1, 0.0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 1, 0.0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 1, 0.0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 1, 0.0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 1, 0.0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 1, 0.0, 0, 0, 0, 0, 0, 0]], {(0, 1): [2.42430642658885, 1, 0, 0, 0], (0, 8): [4.381424409937937, 1, 0, 0, 0], (0, 9): [3.980784521925295, 1, 0, 0, 0], (0, 10): [3.4496610761058837, 1, 0, 0, 0], (1, 2): [1.5440992098955297, 1, 0, 0, 0], (1, 11): [2.1896199419077274, 1, 0, 0, 0], (1, 12): [2.1892814551811286, 1, 0, 0, 0], (2, 3): [2.537537974100092, 0, 1, 0, 0], (2, 4): [2.5163101021138075, 1, 0, 0, 0], (4, 5): [2.500896539243477, 1, 0, 0, 0], (4, 6): [3.9372615991320665, 1, 0, 0, 0], (4, 7): [5.038419288824621, 1, 0, 0, 0], (5, 13): [2.761759781371291, 1, 0, 0, 0], (5, 14): [1.0952199231204662, 1, 0, 0, 0], (5, 15): [1.0958987772600168, 1, 0, 0, 0], (6, 16): [3.332550112151354, 1, 0, 0, 0], (6, 17): [1.093982111371114, 1, 0, 0, 0], (6, 18): [1.0922189707196996, 1, 0, 0, 0], (7, 19): [1.095201830714321, 1, 0, 0, 0], (7, 20): [1.094420814860536, 1, 0, 0, 0], (7, 21): [1.0947931402781075, 1, 0, 0, 0]}), [13.4711, 1.44])\n",
      "\n",
      "\n",
      "((matrix([[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1.,\n",
      "         0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]), [[0, 1, 0, 0, 0, 6, -0.48, 0, 0, 0, 0, 0, 1], [0, 0, 0, 1, 0, 8, -0.68, 1, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 15, 1.47, 0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 16, -0.57, 0, 0, 0, 0, 1, 0], [0, 0, 0, 1, 0, 8, -0.57, 1, 0, 0, 0, 0, 1], [0, 1, 0, 0, 0, 6, -0.55, 0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 16, -0.55, 0, 0, 0, 0, 0, 1], [0, 1, 0, 0, 0, 6, -0.42, 0, 0, 0, 0, 0, 1], [0, 0, 1, 0, 0, 7, 0.09, 0, 1, 0, 0, 1, 0], [0, 1, 0, 0, 0, 6, 0.09, 0, 0, 0, 0, 1, 0], [0, 0, 0, 1, 0, 8, 0.54, 1, 0, 0, 0, 1, 0], [0, 1, 0, 0, 0, 6, 0.54, 0, 0, 1, 0, 1, 0], [0, 1, 0, 0, 0, 6, 0.53, 0, 0, 1, 0, 1, 0], [0, 1, 0, 0, 0, 6, -0.15, 0, 0, 1, 0, 1, 0], [0, 1, 0, 0, 0, 6, -0.15, 0, 0, 1, 0, 1, 0], [0, 1, 0, 0, 0, 6, -0.15, 0, 0, 1, 0, 1, 0], [0, 1, 0, 0, 0, 6, -0.15, 0, 0, 1, 0, 1, 0], [0, 1, 0, 0, 0, 6, 0.28, 0, 0, 0, 0, 1, 0], [0, 0, 0, 1, 0, 8, 0.28, 1, 0, 0, 0, 1, 0], [1, 0, 0, 0, 0, 1, 0.0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 1, 0.0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 1, 0.15, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 1, 0.15, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 1, 0.15, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 1, 0.15, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 1, 0.0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 1, 0.0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 1, 0.0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 1, 0.0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 1, 0.0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 1, 0.0, 0, 0, 0, 0, 0, 0]], {(0, 1): [3.5178341149065004, 1, 0, 0, 0], (0, 19): [2.3954658106514484, 1, 0, 0, 0], (0, 20): [2.3920154117396484, 1, 0, 0, 0], (0, 21): [6.206556456683528, 1, 0, 0, 0], (1, 2): [1.9620919983527787, 1, 0, 0, 0], (2, 3): [5.400225033274077, 0, 1, 0, 0], (2, 4): [5.382806332202563, 1, 0, 0, 0], (2, 6): [1.6344099241010501, 1, 0, 0, 0], (4, 5): [5.1639861366971145, 1, 0, 0, 0], (5, 22): [8.035863702801336, 1, 0, 0, 0], (5, 23): [10.424403252464861, 1, 0, 0, 0], (5, 24): [10.094595780911687, 1, 0, 0, 0], (6, 7): [4.713370582714667, 1, 0, 0, 0], (7, 8): [2.264595188990739, 1, 0, 0, 0], (7, 25): [5.603937774458243, 1, 0, 0, 0], (7, 26): [6.31338125017015, 1, 0, 0, 0], (8, 9): [1.3869338160128626, 1, 0, 0, 0], (8, 17): [7.509026101965553, 1, 0, 0, 0], (9, 10): [2.3063935657211676, 0, 1, 0, 0], (9, 11): [1.462646519156286, 1, 0, 0, 0], (11, 12): [2.494189748194792, 0, 0, 0, 1], (11, 16): [3.785861473958074, 0, 0, 0, 1], (12, 13): [4.983524080407357, 0, 0, 0, 1], (12, 27): [3.8626595759916507, 1, 0, 0, 0], (13, 14): [2.847370044444522, 0, 0, 0, 1], (13, 28): [7.0104440472768905, 1, 0, 0, 0], (14, 15): [2.4430313239907506, 0, 0, 0, 1], (14, 29): [9.655833955179636, 1, 0, 0, 0], (15, 16): [1.4100018865235606, 0, 0, 0, 1], (15, 30): [9.154530000497022, 1, 0, 0, 0], (16, 17): [9.268351605868219, 1, 0, 0, 0], (17, 18): [5.155201320026213, 0, 1, 0, 0]}), [31.6919, 3.675])\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "PicklingError",
     "evalue": "Could not pickle the task to send it to the workers.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"C:\\Users\\vkrin\\Anaconda3\\lib\\site-packages\\joblib\\externals\\loky\\backend\\queues.py\", line 150, in _feed\n    obj_ = dumps(obj, reducers=reducers)\n  File \"C:\\Users\\vkrin\\Anaconda3\\lib\\site-packages\\joblib\\externals\\loky\\backend\\reduction.py\", line 243, in dumps\n    dump(obj, buf, reducers=reducers, protocol=protocol)\n  File \"C:\\Users\\vkrin\\Anaconda3\\lib\\site-packages\\joblib\\externals\\loky\\backend\\reduction.py\", line 236, in dump\n    _LokyPickler(file, reducers=reducers, protocol=protocol).dump(obj)\n  File \"C:\\Users\\vkrin\\Anaconda3\\lib\\site-packages\\joblib\\externals\\cloudpickle\\cloudpickle.py\", line 267, in dump\n    return Pickler.dump(self, obj)\n  File \"C:\\Users\\vkrin\\Anaconda3\\lib\\pickle.py\", line 437, in dump\n    self.save(obj)\n  File \"C:\\Users\\vkrin\\Anaconda3\\lib\\pickle.py\", line 549, in save\n    self.save_reduce(obj=obj, *rv)\n  File \"C:\\Users\\vkrin\\Anaconda3\\lib\\pickle.py\", line 662, in save_reduce\n    save(state)\n  File \"C:\\Users\\vkrin\\Anaconda3\\lib\\pickle.py\", line 504, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"C:\\Users\\vkrin\\Anaconda3\\lib\\pickle.py\", line 859, in save_dict\n    self._batch_setitems(obj.items())\n  File \"C:\\Users\\vkrin\\Anaconda3\\lib\\pickle.py\", line 885, in _batch_setitems\n    save(v)\n  File \"C:\\Users\\vkrin\\Anaconda3\\lib\\pickle.py\", line 549, in save\n    self.save_reduce(obj=obj, *rv)\n  File \"C:\\Users\\vkrin\\Anaconda3\\lib\\pickle.py\", line 662, in save_reduce\n    save(state)\n  File \"C:\\Users\\vkrin\\Anaconda3\\lib\\pickle.py\", line 504, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"C:\\Users\\vkrin\\Anaconda3\\lib\\pickle.py\", line 859, in save_dict\n    self._batch_setitems(obj.items())\n  File \"C:\\Users\\vkrin\\Anaconda3\\lib\\pickle.py\", line 890, in _batch_setitems\n    save(v)\n  File \"C:\\Users\\vkrin\\Anaconda3\\lib\\pickle.py\", line 549, in save\n    self.save_reduce(obj=obj, *rv)\n  File \"C:\\Users\\vkrin\\Anaconda3\\lib\\pickle.py\", line 662, in save_reduce\n    save(state)\n  File \"C:\\Users\\vkrin\\Anaconda3\\lib\\pickle.py\", line 504, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"C:\\Users\\vkrin\\Anaconda3\\lib\\pickle.py\", line 859, in save_dict\n    self._batch_setitems(obj.items())\n  File \"C:\\Users\\vkrin\\Anaconda3\\lib\\pickle.py\", line 885, in _batch_setitems\n    save(v)\n  File \"C:\\Users\\vkrin\\Anaconda3\\lib\\pickle.py\", line 504, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"C:\\Users\\vkrin\\Anaconda3\\lib\\pickle.py\", line 819, in save_list\n    self._batch_appends(obj)\n  File \"C:\\Users\\vkrin\\Anaconda3\\lib\\pickle.py\", line 846, in _batch_appends\n    save(tmp[0])\n  File \"C:\\Users\\vkrin\\Anaconda3\\lib\\pickle.py\", line 504, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"C:\\Users\\vkrin\\Anaconda3\\lib\\pickle.py\", line 774, in save_tuple\n    save(element)\n  File \"C:\\Users\\vkrin\\Anaconda3\\lib\\pickle.py\", line 504, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"C:\\Users\\vkrin\\Anaconda3\\lib\\pickle.py\", line 789, in save_tuple\n    save(element)\n  File \"C:\\Users\\vkrin\\Anaconda3\\lib\\pickle.py\", line 549, in save\n    self.save_reduce(obj=obj, *rv)\n  File \"C:\\Users\\vkrin\\Anaconda3\\lib\\pickle.py\", line 633, in save_reduce\n    save(cls)\n  File \"C:\\Users\\vkrin\\Anaconda3\\lib\\pickle.py\", line 504, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"C:\\Users\\vkrin\\Anaconda3\\lib\\site-packages\\joblib\\externals\\cloudpickle\\cloudpickle.py\", line 693, in save_global\n    return self.save_dynamic_class(obj)\n  File \"C:\\Users\\vkrin\\Anaconda3\\lib\\site-packages\\joblib\\externals\\cloudpickle\\cloudpickle.py\", line 532, in save_dynamic_class\n    save(clsdict)\n  File \"C:\\Users\\vkrin\\Anaconda3\\lib\\pickle.py\", line 504, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"C:\\Users\\vkrin\\Anaconda3\\lib\\pickle.py\", line 859, in save_dict\n    self._batch_setitems(obj.items())\n  File \"C:\\Users\\vkrin\\Anaconda3\\lib\\pickle.py\", line 885, in _batch_setitems\n    save(v)\n  File \"C:\\Users\\vkrin\\Anaconda3\\lib\\pickle.py\", line 504, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"C:\\Users\\vkrin\\Anaconda3\\lib\\site-packages\\joblib\\externals\\cloudpickle\\cloudpickle.py\", line 395, in save_function\n    self.save_function_tuple(obj)\n  File \"C:\\Users\\vkrin\\Anaconda3\\lib\\site-packages\\joblib\\externals\\cloudpickle\\cloudpickle.py\", line 594, in save_function_tuple\n    save(state)\n  File \"C:\\Users\\vkrin\\Anaconda3\\lib\\pickle.py\", line 504, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"C:\\Users\\vkrin\\Anaconda3\\lib\\pickle.py\", line 859, in save_dict\n    self._batch_setitems(obj.items())\n  File \"C:\\Users\\vkrin\\Anaconda3\\lib\\pickle.py\", line 885, in _batch_setitems\n    save(v)\n  File \"C:\\Users\\vkrin\\Anaconda3\\lib\\pickle.py\", line 504, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"C:\\Users\\vkrin\\Anaconda3\\lib\\pickle.py\", line 859, in save_dict\n    self._batch_setitems(obj.items())\n  File \"C:\\Users\\vkrin\\Anaconda3\\lib\\pickle.py\", line 885, in _batch_setitems\n    save(v)\n  File \"C:\\Users\\vkrin\\Anaconda3\\lib\\pickle.py\", line 504, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"C:\\Users\\vkrin\\Anaconda3\\lib\\site-packages\\joblib\\externals\\cloudpickle\\cloudpickle.py\", line 395, in save_function\n    self.save_function_tuple(obj)\n  File \"C:\\Users\\vkrin\\Anaconda3\\lib\\site-packages\\joblib\\externals\\cloudpickle\\cloudpickle.py\", line 594, in save_function_tuple\n    save(state)\n  File \"C:\\Users\\vkrin\\Anaconda3\\lib\\pickle.py\", line 504, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"C:\\Users\\vkrin\\Anaconda3\\lib\\pickle.py\", line 859, in save_dict\n    self._batch_setitems(obj.items())\n  File \"C:\\Users\\vkrin\\Anaconda3\\lib\\pickle.py\", line 885, in _batch_setitems\n    save(v)\n  File \"C:\\Users\\vkrin\\Anaconda3\\lib\\pickle.py\", line 504, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"C:\\Users\\vkrin\\Anaconda3\\lib\\pickle.py\", line 859, in save_dict\n    self._batch_setitems(obj.items())\n  File \"C:\\Users\\vkrin\\Anaconda3\\lib\\pickle.py\", line 885, in _batch_setitems\n    save(v)\n  File \"C:\\Users\\vkrin\\Anaconda3\\lib\\pickle.py\", line 549, in save\n    self.save_reduce(obj=obj, *rv)\n  File \"C:\\Users\\vkrin\\Anaconda3\\lib\\pickle.py\", line 662, in save_reduce\n    save(state)\n  File \"C:\\Users\\vkrin\\Anaconda3\\lib\\pickle.py\", line 504, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"C:\\Users\\vkrin\\Anaconda3\\lib\\pickle.py\", line 859, in save_dict\n    self._batch_setitems(obj.items())\n  File \"C:\\Users\\vkrin\\Anaconda3\\lib\\pickle.py\", line 890, in _batch_setitems\n    save(v)\n  File \"C:\\Users\\vkrin\\Anaconda3\\lib\\pickle.py\", line 549, in save\n    self.save_reduce(obj=obj, *rv)\n  File \"C:\\Users\\vkrin\\Anaconda3\\lib\\pickle.py\", line 662, in save_reduce\n    save(state)\n  File \"C:\\Users\\vkrin\\Anaconda3\\lib\\pickle.py\", line 504, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"C:\\Users\\vkrin\\Anaconda3\\lib\\pickle.py\", line 859, in save_dict\n    self._batch_setitems(obj.items())\n  File \"C:\\Users\\vkrin\\Anaconda3\\lib\\pickle.py\", line 885, in _batch_setitems\n    save(v)\n  File \"C:\\Users\\vkrin\\Anaconda3\\lib\\pickle.py\", line 504, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"C:\\Users\\vkrin\\Anaconda3\\lib\\pickle.py\", line 859, in save_dict\n    self._batch_setitems(obj.items())\n  File \"C:\\Users\\vkrin\\Anaconda3\\lib\\pickle.py\", line 885, in _batch_setitems\n    save(v)\n  File \"C:\\Users\\vkrin\\Anaconda3\\lib\\pickle.py\", line 549, in save\n    self.save_reduce(obj=obj, *rv)\n  File \"C:\\Users\\vkrin\\Anaconda3\\lib\\pickle.py\", line 662, in save_reduce\n    save(state)\n  File \"C:\\Users\\vkrin\\Anaconda3\\lib\\pickle.py\", line 504, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"C:\\Users\\vkrin\\Anaconda3\\lib\\pickle.py\", line 859, in save_dict\n    self._batch_setitems(obj.items())\n  File \"C:\\Users\\vkrin\\Anaconda3\\lib\\pickle.py\", line 885, in _batch_setitems\n    save(v)\n  File \"C:\\Users\\vkrin\\Anaconda3\\lib\\pickle.py\", line 504, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"C:\\Users\\vkrin\\Anaconda3\\lib\\pickle.py\", line 859, in save_dict\n    self._batch_setitems(obj.items())\n  File \"C:\\Users\\vkrin\\Anaconda3\\lib\\pickle.py\", line 885, in _batch_setitems\n    save(v)\n  File \"C:\\Users\\vkrin\\Anaconda3\\lib\\pickle.py\", line 504, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"C:\\Users\\vkrin\\Anaconda3\\lib\\site-packages\\joblib\\externals\\cloudpickle\\cloudpickle.py\", line 674, in save_builtin_function\n    return self.save_function(obj)\n  File \"C:\\Users\\vkrin\\Anaconda3\\lib\\site-packages\\joblib\\externals\\cloudpickle\\cloudpickle.py\", line 387, in save_function\n    return self.save_reduce(obj=obj, *rv)\n  File \"C:\\Users\\vkrin\\Anaconda3\\lib\\pickle.py\", line 572, in save_reduce\n    raise PicklingError(\"args from save_reduce() must be a tuple\")\n_pickle.PicklingError: args from save_reduce() must be a tuple\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mPicklingError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-88906b157352>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_graph_stats\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_valid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'degrees'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Time Statistics Par'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-fa52e7e8f918>\u001b[0m in \u001b[0;36mget_graph_stats\u001b[1;34m(graph_obj_handle, prop)\u001b[0m\n\u001b[0;32m    116\u001b[0m     \u001b[0mnum_cores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmultiprocessing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m     \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgraph_obj_handle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mnum_cores\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_cores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgraph_obj_handle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 118\u001b[1;33m     \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mParallel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_cores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdelayed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_values\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgraph_obj_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprop\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_cores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    119\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m     \u001b[0mstat_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    932\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    933\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 934\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    935\u001b[0m             \u001b[1;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    936\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    831\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    832\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'supports_timeout'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 833\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    834\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    835\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[1;34m(future, timeout)\u001b[0m\n\u001b[0;32m    519\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[0;32m    520\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 521\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    522\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mLokyTimeoutError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    523\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\concurrent\\futures\\_base.py\u001b[0m in \u001b[0;36mresult\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    433\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    434\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 435\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    436\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    437\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\concurrent\\futures\\_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    382\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__get_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    383\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 384\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    385\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    386\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPicklingError\u001b[0m: Could not pickle the task to send it to the workers."
     ]
    }
   ],
   "source": [
    "# Parse optios for downloading\n",
    "parser = argparse.ArgumentParser(description='QM9 Object.')\n",
    "# Optional argument\n",
    "parser.add_argument('--root', nargs=1, help='Specify the data directory.', default=['data-parse/chemfindata/'])\n",
    "\n",
    "args = parser.parse_args([])\n",
    "root = args.root[0]\n",
    "\n",
    "files = [f for f in os.listdir(root) if os.path.isfile(os.path.join(root, f))]\n",
    "\n",
    "idx = np.random.permutation(len(files))\n",
    "idx = idx.tolist()\n",
    "\n",
    "valid_ids = [files[i] for i in idx[0:1000]]\n",
    "test_ids  = [files[i] for i in idx[1000:2000]]\n",
    "train_ids = [files[i] for i in idx[2000:]]\n",
    "\n",
    "data_train = Qm9(root, train_ids, )\n",
    "data_valid = Qm9(root, valid_ids)\n",
    "data_test = Qm9(root, test_ids)\n",
    "\n",
    "print(len(data_train))\n",
    "print(len(data_valid))\n",
    "print(len(data_test))\n",
    "\n",
    "print(data_train[0])\n",
    "print('\\n')\n",
    "print(data_valid[0])\n",
    "print('\\n')\n",
    "print(data_test[0])\n",
    "print('\\n')\n",
    "\n",
    "start = time.time()\n",
    "print(get_graph_stats(data_valid, 'degrees'))\n",
    "end = time.time()\n",
    "print('Time Statistics Par')\n",
    "print(end - start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def restricted_float(x, inter):\n",
    "    x = float(x)\n",
    "    if x < inter[0] or x > inter[1]:\n",
    "        raise argparse.ArgumentTypeError(\"%r not in range [1e-5, 1e-4]\"%(x,))\n",
    "    return x\n",
    "\n",
    "# Argument parser\n",
    "parser = argparse.ArgumentParser(description='Neural message passing')\n",
    "\n",
    "parser.add_argument('--dataset', default='our_data')\n",
    "parser.add_argument('--datasetPath', default='./data-parse/chemfindata/', help='dataset path')\n",
    "parser.add_argument('--logPath', default='./log/our_data/mpnn/', help='log path')\n",
    "parser.add_argument('--plotLr', default=False, help='allow plotting the data')\n",
    "parser.add_argument('--plotPath', default='./plot/our_data/mpnn/', help='plot path')\n",
    "parser.add_argument('--resume', default='./checkpoint/our_data/mpnn/',\n",
    "                    help='path to latest checkpoint')\n",
    "# Optimization Options\n",
    "parser.add_argument('--batch-size', type=int, default=20, metavar='N',\n",
    "                    help='Input batch size for training (default: 20)')\n",
    "parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "                    help='Enables CUDA training')\n",
    "parser.add_argument('--epochs', type=int, default=360, metavar='N',\n",
    "                    help='Number of epochs to train (default: 360)')\n",
    "parser.add_argument('--lr', type=lambda x: restricted_float(x, [1e-5, 1e-2]), default=1e-3, metavar='LR',\n",
    "                    help='Initial learning rate [1e-5, 5e-4] (default: 1e-4)')\n",
    "parser.add_argument('--lr-decay', type=lambda x: restricted_float(x, [.01, 1]), default=0.6, metavar='LR-DECAY',\n",
    "                    help='Learning rate decay factor [.01, 1] (default: 0.6)')\n",
    "parser.add_argument('--schedule', type=list, default=[0.1, 0.9], metavar='S',\n",
    "                    help='Percentage of epochs to start the learning rate decay [0, 1] (default: [0.1, 0.9])')\n",
    "parser.add_argument('--momentum', type=float, default=0.9, metavar='M',\n",
    "                    help='SGD momentum (default: 0.9)')\n",
    "# i/o\n",
    "parser.add_argument('--log-interval', type=int, default=20, metavar='N',\n",
    "                    help='How many batches to wait before logging training status')\n",
    "# Accelerating\n",
    "parser.add_argument('--prefetch', type=int, default=2, help='Pre-fetching threads.')\n",
    "\n",
    "best_er1 = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_dict = get_graph_stats(data_valid, ['target_mean', 'target_std'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = parser.parse_args([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Define model\n",
      "\tStatistics\n"
     ]
    }
   ],
   "source": [
    "print('Define model')\n",
    "# Select one graph\n",
    "g_tuple, l = data_train[0]\n",
    "g, h_t, e = g_tuple\n",
    "\n",
    "print('\\tStatistics')\n",
    "stat_dict = get_graph_stats(data_valid, ['target_mean', 'target_std'])\n",
    "\n",
    "data_train.set_target_transform(lambda x: datasets.utils.normalize_data(x,stat_dict['target_mean'],stat_dict['target_std']))\n",
    "data_valid.set_target_transform(lambda x: datasets.utils.normalize_data(x, stat_dict['target_mean'],stat_dict['target_std']))\n",
    "data_test.set_target_transform(lambda x: datasets.utils.normalize_data(x, stat_dict['target_mean'], stat_dict['target_std']))\n",
    "\n",
    "# Data Loader\n",
    "train_loader = torch.utils.data.DataLoader(data_train,\n",
    "                                           batch_size=args.batch_size, shuffle=True,\n",
    "                                           collate_fn=datasets.utils.collate_g,\n",
    "                                        pin_memory=True)\n",
    "valid_loader = torch.utils.data.DataLoader(data_valid,\n",
    "                                           batch_size=args.batch_size, collate_fn=datasets.utils.collate_g,\n",
    "                                           pin_memory=True)\n",
    "test_loader = torch.utils.data.DataLoader(data_test,\n",
    "                                          batch_size=args.batch_size, collate_fn=datasets.utils.collate_g,\n",
    "                                          pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.cuda =None# not args.no_cuda and torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tCreate model\n",
      "Optimizer\n",
      "=> loading best model './checkpoint/our_data/mpnn/model_best.pth'\n",
      "=> loaded best model './checkpoint/our_data/mpnn/model_best.pth' (epoch 11)\n",
      "Check cuda\n"
     ]
    }
   ],
   "source": [
    "print('\\tCreate model')\n",
    "in_n = [len(h_t[0]), len(list(e.values())[0])]\n",
    "hidden_state_size = 73\n",
    "message_size = 73\n",
    "n_layers = 3\n",
    "l_target = len(l)\n",
    "type ='regression'\n",
    "model = MPNN(in_n, hidden_state_size, message_size, n_layers, l_target, type=type)\n",
    "del in_n, hidden_state_size, message_size, n_layers, l_target, type\n",
    "\n",
    "print('Optimizer')\n",
    "optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "evaluation = lambda output, target: torch.mean(torch.abs(output - target) / torch.abs(target))\n",
    "\n",
    "\n",
    "lr_step = (args.lr-args.lr*args.lr_decay)/(args.epochs*args.schedule[1] - args.epochs*args.schedule[0])\n",
    "\n",
    "# get the best checkpoint if available without training\n",
    "if args.resume:\n",
    "    checkpoint_dir = args.resume\n",
    "    best_model_file = os.path.join(checkpoint_dir, 'model_best.pth')\n",
    "    if not os.path.isdir(checkpoint_dir):\n",
    "        os.makedirs(checkpoint_dir)\n",
    "    if os.path.isfile(best_model_file):\n",
    "        print(\"=> loading best model '{}'\".format(best_model_file))\n",
    "        checkpoint = torch.load(best_model_file)\n",
    "        args.start_epoch = checkpoint['epoch']\n",
    "        best_acc1 = checkpoint['best_er1']\n",
    "        model.load_state_dict(checkpoint['state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        print(\"=> loaded best model '{}' (epoch {})\".format(best_model_file, checkpoint['epoch']))\n",
    "    else:\n",
    "        print(\"=> no best model found at '{}'\".format(best_model_file))\n",
    "\n",
    "print('Check cuda')\n",
    "if args.cuda:\n",
    "    print('\\t* Cuda')\n",
    "    model = model.cuda()\n",
    "    criterion = criterion.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MPNN(\n",
       "  (m): ModuleList(\n",
       "    (0): MessageFunction(\n",
       "      (learn_args): ParameterList()\n",
       "      (learn_modules): ModuleList(\n",
       "        (0): NNet(\n",
       "          (fcs): ModuleList(\n",
       "            (0): Linear(in_features=5, out_features=128, bias=True)\n",
       "            (1): Linear(in_features=128, out_features=256, bias=True)\n",
       "            (2): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (3): Linear(in_features=128, out_features=5329, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (u): ModuleList(\n",
       "    (0): UpdateFunction(\n",
       "      (learn_args): ParameterList()\n",
       "      (learn_modules): ModuleList(\n",
       "        (0): GRU(73, 73)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (r): ReadoutFunction(\n",
       "    (learn_args): ParameterList()\n",
       "    (learn_modules): ModuleList(\n",
       "      (0): NNet(\n",
       "        (fcs): ModuleList(\n",
       "          (0): Linear(in_features=146, out_features=128, bias=True)\n",
       "          (1): Linear(in_features=128, out_features=256, bias=True)\n",
       "          (2): Linear(in_features=256, out_features=128, bias=True)\n",
       "          (3): Linear(in_features=128, out_features=2, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (1): NNet(\n",
       "        (fcs): ModuleList(\n",
       "          (0): Linear(in_features=73, out_features=128, bias=True)\n",
       "          (1): Linear(in_features=128, out_features=256, bias=True)\n",
       "          (2): Linear(in_features=256, out_features=128, bias=True)\n",
       "          (3): Linear(in_features=128, out_features=2, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model, criterion, optimizer, epoch, evaluation, logger):\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    error_ratio = AverageMeter()\n",
    "\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "\n",
    "    end = time.time()\n",
    "    for i, (g, h, e, target) in enumerate(train_loader):\n",
    "\n",
    "        # Prepare input data\n",
    "        if args.cuda:\n",
    "            g, h, e, target = g.cuda(), h.cuda(), e.cuda(), target.cuda()\n",
    "        g, h, e, target = Variable(g), Variable(h), Variable(e), Variable(target)\n",
    "\n",
    "        # Measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Compute output\n",
    "        output = model(g, h, e)\n",
    "        train_loss = criterion(output, target)\n",
    "\n",
    "        # Logs\n",
    "        losses.update(train_loss.item(), g.size(0))\n",
    "        error_ratio.update(evaluation(output, target).item(), g.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if i % args.log_interval == 0 and i > 0:\n",
    "\n",
    "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Error Ratio {err.val:.4f} ({err.avg:.4f})'\n",
    "                  .format(epoch, i, len(train_loader), batch_time=batch_time,\n",
    "                          data_time=data_time, loss=losses, err=error_ratio))\n",
    "                          \n",
    "\n",
    "    print('Epoch: [{0}] Avg Error Ratio {err.avg:.3f}; Average Loss {loss.avg:.3f}; Avg Time x Batch {b_time.avg:.3f}'\n",
    "          .format(epoch, err=error_ratio, loss=losses, b_time=batch_time))\n",
    "\n",
    "\n",
    "def validate(val_loader, model, criterion, evaluation, logger=None):\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    error_ratio = AverageMeter()\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    end = time.time()\n",
    "    for i, (g, h, e, target) in enumerate(val_loader):\n",
    "\n",
    "        # Prepare input data\n",
    "        if args.cuda:\n",
    "            g, h, e, target = g.cuda(), h.cuda(), e.cuda(), target.cuda()\n",
    "        g, h, e, target = Variable(g), Variable(h), Variable(e), Variable(target)\n",
    "\n",
    "        # Compute output\n",
    "        output = model(g, h, e)\n",
    "\n",
    "        # Logs\n",
    "        losses.update(criterion(output, target).item(), g.size(0))\n",
    "        error_ratio.update(evaluation(output, target).item(), g.size(0))\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if i % args.log_interval == 0 and i > 0:\n",
    "            \n",
    "            print('Test: [{0}/{1}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Error Ratio {err.val:.4f} ({err.avg:.4f})'\n",
    "                  .format(i, len(val_loader), batch_time=batch_time,\n",
    "                          loss=losses, err=error_ratio))\n",
    "\n",
    "    print(' * Average Error Ratio {err.avg:.3f}; Average Loss {loss.avg:.3f}'\n",
    "          .format(err=error_ratio, loss=losses))\n",
    "\n",
    "\n",
    "    return error_ratio.avg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0][20/68]\tTime 3.070 (4.912)\tData 0.392 (0.416)\tLoss 0.4190 (0.4555)\tError Ratio 0.8111 (2.7604)\n",
      "Epoch: [0][40/68]\tTime 3.489 (4.631)\tData 0.386 (0.410)\tLoss 0.2396 (0.6721)\tError Ratio 1.0590 (1.9948)\n",
      "Epoch: [0][60/68]\tTime 5.629 (4.664)\tData 0.422 (0.412)\tLoss 0.4082 (0.7152)\tError Ratio 1.0468 (1.7517)\n",
      "Epoch: [0] Avg Error Ratio 1.710; Average Loss 0.697; Avg Time x Batch 4.574\n",
      "Test: [20/50]\tTime 1.362 (1.827)\tLoss 1.5316 (0.7793)\tError Ratio 0.7588 (2.1140)\n",
      "Test: [40/50]\tTime 2.400 (2.243)\tLoss 0.4271 (0.8062)\tError Ratio 0.9512 (3.6430)\n",
      " * Average Error Ratio 3.224; Average Loss 0.728\n",
      "Epoch: [1][20/68]\tTime 4.745 (4.430)\tData 0.446 (0.401)\tLoss 0.2681 (0.5095)\tError Ratio 0.8766 (1.2672)\n",
      "Epoch: [1][40/68]\tTime 5.469 (4.635)\tData 0.442 (0.409)\tLoss 0.3070 (0.7446)\tError Ratio 0.8496 (1.3444)\n",
      "Epoch: [1][60/68]\tTime 4.174 (4.897)\tData 0.368 (0.416)\tLoss 1.6426 (0.6715)\tError Ratio 1.5743 (1.3916)\n",
      "Epoch: [1] Avg Error Ratio 1.382; Average Loss 0.660; Avg Time x Batch 4.723\n",
      "Test: [20/50]\tTime 1.514 (1.781)\tLoss 1.6393 (0.8042)\tError Ratio 0.7298 (2.0145)\n",
      "Test: [40/50]\tTime 1.930 (2.072)\tLoss 0.4209 (0.8522)\tError Ratio 1.0161 (2.1136)\n",
      " * Average Error Ratio 1.958; Average Loss 0.761\n",
      "Epoch: [2][20/68]\tTime 3.117 (5.439)\tData 0.370 (0.423)\tLoss 0.3384 (0.9871)\tError Ratio 1.0371 (2.3254)\n",
      "Epoch: [2][40/68]\tTime 5.957 (5.327)\tData 0.484 (0.437)\tLoss 0.2653 (0.7538)\tError Ratio 0.8139 (1.8486)\n",
      "Epoch: [2][60/68]\tTime 5.588 (4.853)\tData 0.415 (0.419)\tLoss 0.3524 (0.6598)\tError Ratio 0.9218 (1.6360)\n",
      "Epoch: [2] Avg Error Ratio 1.583; Average Loss 0.649; Avg Time x Batch 4.734\n",
      "Test: [20/50]\tTime 1.363 (1.734)\tLoss 1.5198 (0.7592)\tError Ratio 0.7073 (1.6553)\n",
      "Test: [40/50]\tTime 2.068 (2.007)\tLoss 0.3982 (0.8011)\tError Ratio 0.9094 (1.9915)\n",
      " * Average Error Ratio 1.864; Average Loss 0.713\n",
      "Epoch: [3][20/68]\tTime 10.750 (4.467)\tData 0.447 (0.388)\tLoss 0.3422 (0.4882)\tError Ratio 1.3538 (1.2612)\n",
      "Epoch: [3][40/68]\tTime 4.163 (4.557)\tData 0.404 (0.391)\tLoss 1.2538 (0.6542)\tError Ratio 2.0320 (1.3521)\n",
      "Epoch: [3][60/68]\tTime 4.296 (4.342)\tData 0.344 (0.388)\tLoss 0.6106 (0.6677)\tError Ratio 0.7843 (1.2923)\n",
      "Epoch: [3] Avg Error Ratio 1.271; Average Loss 0.633; Avg Time x Batch 4.260\n",
      "Test: [20/50]\tTime 1.374 (1.705)\tLoss 1.2629 (0.7358)\tError Ratio 0.7770 (1.9723)\n",
      "Test: [40/50]\tTime 1.834 (1.950)\tLoss 0.3343 (0.7575)\tError Ratio 0.7959 (2.7811)\n",
      " * Average Error Ratio 2.474; Average Loss 0.679\n",
      "Epoch: [4][20/68]\tTime 3.990 (4.026)\tData 0.426 (0.377)\tLoss 0.2321 (1.0138)\tError Ratio 1.2349 (1.1253)\n",
      "Epoch: [4][40/68]\tTime 5.304 (4.277)\tData 0.390 (0.390)\tLoss 0.2149 (0.7126)\tError Ratio 2.2137 (1.1752)\n",
      "Epoch: [4][60/68]\tTime 2.380 (4.256)\tData 0.360 (0.387)\tLoss 0.3352 (0.6374)\tError Ratio 1.4834 (1.4807)\n",
      "Epoch: [4] Avg Error Ratio 1.465; Average Loss 0.627; Avg Time x Batch 4.290\n",
      "Test: [20/50]\tTime 1.297 (1.662)\tLoss 1.2555 (0.7279)\tError Ratio 0.8410 (1.6946)\n",
      "Test: [40/50]\tTime 1.855 (2.009)\tLoss 0.3632 (0.7567)\tError Ratio 0.8604 (2.3979)\n",
      " * Average Error Ratio 2.150; Average Loss 0.680\n",
      "Epoch: [5][20/68]\tTime 3.257 (4.098)\tData 0.368 (0.377)\tLoss 0.2362 (0.6100)\tError Ratio 2.0036 (1.0695)\n",
      "Epoch: [5][40/68]\tTime 3.913 (4.074)\tData 0.347 (0.386)\tLoss 1.4164 (0.5186)\tError Ratio 1.2311 (1.2508)\n",
      "Epoch: [5][60/68]\tTime 4.388 (4.262)\tData 0.432 (0.392)\tLoss 2.1067 (0.6339)\tError Ratio 1.0143 (1.4946)\n",
      "Epoch: [5] Avg Error Ratio 1.481; Average Loss 0.611; Avg Time x Batch 4.382\n",
      "Test: [20/50]\tTime 1.372 (1.686)\tLoss 1.2676 (0.7513)\tError Ratio 0.8922 (1.9847)\n",
      "Test: [40/50]\tTime 2.318 (2.067)\tLoss 0.3348 (0.7774)\tError Ratio 0.9030 (4.1871)\n",
      " * Average Error Ratio 3.676; Average Loss 0.702\n",
      "Epoch: [6][20/68]\tTime 6.448 (4.421)\tData 0.398 (0.436)\tLoss 0.7786 (0.6723)\tError Ratio 2.4404 (1.4962)\n",
      "Epoch: [6][40/68]\tTime 12.829 (5.181)\tData 0.485 (0.444)\tLoss 0.8541 (0.6589)\tError Ratio 1.2877 (1.4332)\n",
      "Epoch: [6][60/68]\tTime 5.266 (4.941)\tData 0.410 (0.440)\tLoss 0.1979 (0.6367)\tError Ratio 1.2224 (1.3298)\n",
      "Epoch: [6] Avg Error Ratio 1.345; Average Loss 0.635; Avg Time x Batch 4.920\n",
      "Test: [20/50]\tTime 1.518 (2.117)\tLoss 1.2256 (0.6976)\tError Ratio 0.7455 (1.9519)\n",
      "Test: [40/50]\tTime 2.592 (2.442)\tLoss 0.3419 (0.7348)\tError Ratio 0.8234 (2.6478)\n",
      " * Average Error Ratio 2.406; Average Loss 0.654\n",
      "Epoch: [7][20/68]\tTime 8.086 (4.747)\tData 0.494 (0.414)\tLoss 0.8537 (0.8817)\tError Ratio 1.8094 (1.0989)\n",
      "Epoch: [7][40/68]\tTime 3.471 (4.686)\tData 0.407 (0.433)\tLoss 0.1524 (0.7079)\tError Ratio 1.3795 (2.0255)\n",
      "Epoch: [7][60/68]\tTime 3.528 (4.695)\tData 0.351 (0.429)\tLoss 0.4593 (0.6539)\tError Ratio 0.8563 (1.7867)\n",
      "Epoch: [7] Avg Error Ratio 1.712; Average Loss 0.616; Avg Time x Batch 4.621\n",
      "Test: [20/50]\tTime 1.383 (1.744)\tLoss 1.2542 (0.6994)\tError Ratio 0.7874 (2.0530)\n",
      "Test: [40/50]\tTime 2.261 (2.087)\tLoss 0.3276 (0.7275)\tError Ratio 0.7694 (2.5721)\n",
      " * Average Error Ratio 2.278; Average Loss 0.649\n",
      "Epoch: [8][20/68]\tTime 2.115 (4.169)\tData 0.361 (0.392)\tLoss 0.2259 (0.6486)\tError Ratio 1.2252 (1.0933)\n",
      "Epoch: [8][40/68]\tTime 3.018 (4.372)\tData 0.366 (0.400)\tLoss 0.1984 (0.6083)\tError Ratio 0.8543 (1.1815)\n",
      "Epoch: [8][60/68]\tTime 4.059 (4.483)\tData 0.403 (0.399)\tLoss 0.5393 (0.6251)\tError Ratio 2.1804 (1.1970)\n",
      "Epoch: [8] Avg Error Ratio 1.307; Average Loss 0.606; Avg Time x Batch 4.452\n",
      "Test: [20/50]\tTime 1.368 (1.757)\tLoss 1.1832 (0.6999)\tError Ratio 0.8026 (1.9193)\n",
      "Test: [40/50]\tTime 2.080 (2.062)\tLoss 0.3223 (0.7317)\tError Ratio 0.7871 (2.9617)\n",
      " * Average Error Ratio 2.617; Average Loss 0.657\n",
      "Epoch: [9][20/68]\tTime 5.336 (5.377)\tData 0.382 (0.429)\tLoss 0.8627 (0.5650)\tError Ratio 1.1826 (1.0827)\n",
      "Epoch: [9][40/68]\tTime 2.261 (5.007)\tData 0.356 (0.426)\tLoss 5.1630 (0.7102)\tError Ratio 1.0523 (1.1540)\n",
      "Epoch: [9][60/68]\tTime 7.088 (4.824)\tData 0.578 (0.424)\tLoss 0.3702 (0.6179)\tError Ratio 1.2877 (1.3433)\n",
      "Epoch: [9] Avg Error Ratio 1.334; Average Loss 0.597; Avg Time x Batch 4.641\n",
      "Test: [20/50]\tTime 1.364 (1.759)\tLoss 1.2085 (0.7236)\tError Ratio 0.6680 (2.5698)\n",
      "Test: [40/50]\tTime 2.050 (2.047)\tLoss 0.4055 (0.7681)\tError Ratio 1.0118 (2.2027)\n",
      " * Average Error Ratio 2.097; Average Loss 0.682\n",
      "Epoch: [10][20/68]\tTime 4.268 (3.849)\tData 0.408 (0.433)\tLoss 0.9966 (0.4229)\tError Ratio 1.2751 (1.0738)\n",
      "Epoch: [10][40/68]\tTime 5.007 (4.217)\tData 0.395 (0.414)\tLoss 0.3164 (0.5352)\tError Ratio 1.2624 (1.6540)\n",
      "Epoch: [10][60/68]\tTime 3.017 (4.373)\tData 0.382 (0.412)\tLoss 0.2231 (0.6295)\tError Ratio 5.1859 (1.5392)\n",
      "Epoch: [10] Avg Error Ratio 1.525; Average Loss 0.595; Avg Time x Batch 4.350\n",
      "Test: [20/50]\tTime 1.636 (1.935)\tLoss 1.1542 (0.7041)\tError Ratio 0.7875 (2.4400)\n",
      "Test: [40/50]\tTime 2.166 (2.250)\tLoss 0.3847 (0.7333)\tError Ratio 1.0022 (3.0949)\n",
      " * Average Error Ratio 2.733; Average Loss 0.651\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-330b846b4016>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;31m# train for one epoch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevaluation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogger\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;31m# evaluate on test set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-10-492d77e44378>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(train_loader, model, criterion, optimizer, epoch, evaluation, logger)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[1;31m# Prepare input data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    344\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    345\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 346\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    347\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    348\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-7741f7a9552c>\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0medge_transform\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m             \u001b[0mg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mqm9_edges\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0me_representation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget_transform\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\OneDrive - C\\techno2\\DM2\\project\\datasets\\utils.py\u001b[0m in \u001b[0;36mqm9_edges\u001b[1;34m(g, e_representation)\u001b[0m\n\u001b[0;32m     90\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0medg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mremove_edges\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m         \u001b[0mg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mremove_edge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0medg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 92\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mnx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_numpy_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     93\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\networkx\\convert_matrix.py\u001b[0m in \u001b[0;36mto_numpy_matrix\u001b[1;34m(G, nodelist, dtype, order, multigraph_weight, weight, nonedge)\u001b[0m\n\u001b[0;32m    187\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    188\u001b[0m         \u001b[1;31m# Graph or DiGraph, this is much faster than above\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 189\u001b[1;33m         \u001b[0mM\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnlen\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnlen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    190\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mu\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnbrdict\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mG\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madjacency_iter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    191\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0md\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnbrdict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(0, args.epochs):\n",
    "\n",
    "    if epoch > args.epochs * args.schedule[0] and epoch < args.epochs * args.schedule[1]:\n",
    "        args.lr -= lr_step\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = args.lr\n",
    "\n",
    "    # train for one epoch\n",
    "    train(train_loader, model, criterion, optimizer, epoch, evaluation, logger=None)\n",
    "\n",
    "    # evaluate on test set\n",
    "    er1 = validate(valid_loader, model, criterion, evaluation)\n",
    "\n",
    "    is_best = er1 > best_er1\n",
    "    best_er1 = min(er1, best_er1)\n",
    "    utils.save_checkpoint({'epoch': epoch + 1, 'state_dict': model.state_dict(), 'best_er1': best_er1,\n",
    "                           'optimizer': optimizer.state_dict(), }, is_best=is_best, directory=args.resume)\n",
    "\n",
    "    # Logger step\n",
    "    #logger.log_value('learning_rate', args.lr).step()\n",
    "\n",
    "# get the best checkpoint and test it with test set\n",
    "if args.resume:\n",
    "    checkpoint_dir = args.resume\n",
    "    best_model_file = os.path.join(checkpoint_dir, 'model_best.pth')\n",
    "    if not os.path.isdir(checkpoint_dir):\n",
    "        os.makedirs(checkpoint_dir)\n",
    "    if os.path.isfile(best_model_file):\n",
    "        print(\"=> loading best model '{}'\".format(best_model_file))\n",
    "        checkpoint = torch.load(best_model_file)\n",
    "        args.start_epoch = checkpoint['epoch']\n",
    "        best_acc1 = checkpoint['best_er1']\n",
    "        model.load_state_dict(checkpoint['state_dict'])\n",
    "        if args.cuda:\n",
    "            model.cuda()\n",
    "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        print(\"=> loaded best model '{}' (epoch {})\".format(best_model_file, checkpoint['epoch']))\n",
    "    else:\n",
    "        print(\"=> no best model found at '{}'\".format(best_model_file))\n",
    "\n",
    "# For testing\n",
    "validate(test_loader, model, criterion, evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: [20/50]\tTime 2.103 (1.969)\tLoss 0.1250 (0.6039)\tError Ratio 0.8150 (3.9243)\n",
      "Test: [40/50]\tTime 3.161 (2.227)\tLoss 0.4517 (0.7833)\tError Ratio 1.4968 (2.7592)\n",
      " * Average Error Ratio 2.513; Average Loss 0.750\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.5131032884120943"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validate(test_loader, model, criterion, evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import networkx as nx\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcol\n",
    "import matplotlib.cm as cm\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "\n",
    "__author__ = \"Pau Riba, Anjan Dutta\"\n",
    "__email__ = \"priba@cvc.uab.cat, adutta@cvc.uab.cat\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "   Plots a Graph with the library networkx\n",
    "\"\"\"\n",
    "\n",
    "class Plotter():\n",
    "    # Constructor\n",
    "    def __init__(self, plot_dir = './'):\n",
    "        self.plotdir = plot_dir\n",
    "\n",
    "        if os.path.isdir(plot_dir):\n",
    "            # clean previous logged data under the same directory name\n",
    "            self._remove(plot_dir)\n",
    "\n",
    "        os.makedirs(plot_dir)\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def _remove(path):\n",
    "        \"\"\" param <path> could either be relative or absolute. \"\"\"\n",
    "        if os.path.isfile(path):\n",
    "            os.remove(path)  # remove the file\n",
    "        elif os.path.isdir(path):\n",
    "            import shutil\n",
    "            shutil.rmtree(path)  # remove dir and all contains\n",
    "\n",
    "    def plot_graph(self, am, position=None, cls=None, fig_name='graph.png'):\n",
    "\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "            g = nx.from_numpy_matrix(am)\n",
    "\n",
    "            if position is None:\n",
    "                position=nx.drawing.circular_layout(g)\n",
    "\n",
    "            fig = plt.figure()\n",
    "\n",
    "            if cls is None:\n",
    "                cls='r'\n",
    "            else:\n",
    "                # Make a user-defined colormap.\n",
    "                cm1 = mcol.LinearSegmentedColormap.from_list(\"MyCmapName\", [\"r\", \"b\"])\n",
    "\n",
    "                # Make a normalizer that will map the time values from\n",
    "                # [start_time,end_time+1] -> [0,1].\n",
    "                cnorm = mcol.Normalize(vmin=0, vmax=1)\n",
    "\n",
    "                # Turn these into an object that can be used to map time values to colors and\n",
    "                # can be passed to plt.colorbar().\n",
    "                cpick = cm.ScalarMappable(norm=cnorm, cmap=cm1)\n",
    "                cpick.set_array([])\n",
    "                cls = cpick.to_rgba(cls)\n",
    "                plt.colorbar(cpick, ax=fig.add_subplot(111))\n",
    "\n",
    "\n",
    "            nx.draw(g, pos=position, node_color=cls, ax=fig.add_subplot(111))\n",
    "\n",
    "            fig.savefig(os.path.join(self.plotdir, fig_name))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m=Plotter(plot_dir='/image')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    " for i, (g, h, e, target) in enumerate(train_loader):\n",
    "    g, h, e, target = Variable(g), Variable(h), Variable(e), Variable(target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.4563, -0.7447],\n",
       "        [-0.5977, -0.0359],\n",
       "        [-0.0172, -0.4512],\n",
       "        [-0.6062, -0.7089],\n",
       "        [-0.4702,  0.6776],\n",
       "        [-0.3575, -0.0813]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "output=model(g,h,e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.8727, -0.1155])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.1039, -0.0164], grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-5.8529, grad_fn=<AddBackward0>)\n",
      "tensor(1.6046, grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(output[0])):\n",
    "    print(output[0][i]*stat_dict['target_std'][i]+stat_dict['target_mean'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5159)\n",
      "tensor(1.4800)\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(target[0])):\n",
    "    print(target[0][i]*stat_dict['target_std'][i]+stat_dict['target_mean'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
